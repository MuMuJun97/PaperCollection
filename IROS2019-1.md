"ICRA 2019 Digest Final," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. i-lvii.
doi: 10.1109/ICRA.2019.8793894
Abstract: Presents abstracts for the articles comprising the conference proceedings.
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793894&isnumber=8793254

"Table of contents," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 1-46.
doi: 10.1109/ICRA.2019.8793964
Abstract: Presents the table of contents/splash page of the proceedings record.
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793964&isnumber=8793254

S. Choi and J. Kim, "Trajectory-based Probabilistic Policy Gradient for Learning Locomotion Behaviors," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 1-7.
doi: 10.1109/ICRA.2019.8794207
Abstract: In this paper, we propose a trajectory-based reinforcement learning method named deep latent policy gradient (DLPG) for learning locomotion skills. We define the policy function as a probability distribution over trajectories and train the policy using a deep latent variable model to achieve sample efficient skill learning. We first evaluate the sample efficiency of DLPG compared to the state-of-the-art reinforcement learning methods in simulated environments. Then, we apply the proposed method to a four-legged walking robot named Snapbot to learn three basic locomotion skills of turn left, go straight, and turn right. We demonstrate that, by properly designing two reward functions for curriculum learning, Snapbot successfully learns the desired locomotion skills with moderate sample complexity.
keywords: {control engineering computing;gradient methods;learning (artificial intelligence);legged locomotion;probability;robot programming;moderate sample complexity;trajectory-based probabilistic policy gradient;trajectory-based reinforcement learning method;deep latent policy gradient;DLPG;policy function;probability distribution;deep latent variable model;curriculum learning;locomotion skills;Snapbot;four-legged walking robot;Trajectory;Legged locomotion;Task analysis;Gradient methods;Stochastic processes;Training},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794207&isnumber=8793254

F. Tsang, R. A. Macdonald and S. L. Smith, "Learning Motion Planning Policies in Uncertain Environments through Repeated Task Executions," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 8-14.
doi: 10.1109/ICRA.2019.8793961
Abstract: The ability to navigate uncertain environments from a start to a goal location is a necessity in many applications. While there are many reactive algorithms for online replanning, there has not been much investigation in leveraging past executions of the same navigation task to improve future executions. In this work, we first formalize this problem by introducing the Learned Reactive Planning Problem (LRPP). Second, we propose a method to capture these past executions and from that determine a motion policy to handle obstacles that the robot has seen before. Third, we show from our experiments that using this policy can significantly reduce the execution cost over just using reactive algorithms.
keywords: {collision avoidance;learning (artificial intelligence);mobile robots;motion control;learned reactive planning problem;motion planning policy learning;execution cost;motion policy;navigation task;online replanning;reactive algorithms;goal location;repeated task executions;uncertain environments;Task analysis;Robot sensing systems;Planning;Navigation;Reinforcement learning;Heuristic algorithms},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793961&isnumber=8793254

B. Ivanovic, J. Harrison, A. Sharma, M. Chen and M. Pavone, "BaRC: Backward Reachability Curriculum for Robotic Reinforcement Learning," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 15-21.
doi: 10.1109/ICRA.2019.8794206
Abstract: Model-free Reinforcement Learning (RL) offers an attractive approach to learn control policies for high dimensional systems, but its relatively poor sample complexity often necessitates training in simulated environments. Even in simulation, goal-directed tasks whose natural reward function is sparse remain intractable for state-of-the-art model-free algorithms for continuous control. The bottleneck in these tasks is the prohibitive amount of exploration required to obtain a learning signal from the initial state of the system. In this work, we leverage physical priors in the form of an approximate system dynamics model to design a curriculum for a model-free policy optimization algorithm. Our Backward Reachability Curriculum (BaRC) begins policy training from states that require a small number of actions to accomplish the task, and expands the initial state distribution backwards in a dynamically-consistent manner once the policy optimization algorithm demonstrates sufficient performance. BaRC is general, in that it can accelerate training of any model-free RL algorithm on a broad class of goal-directed continuous control MDPs. Its curriculum strategy is physically intuitive, easy-to-tune, and allows incorporating physical priors to accelerate training without hindering the performance, flexibility, and applicability of the model-free RL algorithm. We evaluate our approach on two representative dynamic robotic learning problems and find substantial performance improvement relative to previous curriculum generation techniques and naive exploration strategies.
keywords: {learning (artificial intelligence);Markov processes;optimisation;path planning;robots;BaRC;initial state distribution backwards;model-free RL algorithm;goal-directed continuous control MDPs;curriculum strategy;representative dynamic robotic learning problems;goal-directed tasks;learning signal;model-free policy optimization algorithm;backward reachability curriculum;curriculum generation techniques;robotic reinforcement learning;model-free reinforcement learning;model-free algorithms;reward function;exploration strategies;Robots;Task analysis;Training;Computational modeling;Heuristic algorithms;Complexity theory;Approximation algorithms},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794206&isnumber=8793254

I. Salehi, G. Yao and A. P. Dani, "Active Sampling based Safe Identification of Dynamical Systems using Extreme Learning Machines and Barrier Certificates," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 22-28.
doi: 10.1109/ICRA.2019.8793891
Abstract: Learning the dynamical system (DS) model from data that preserves dynamical system properties is an important problem in many robot learning applications. Typically, the joint data coming from cyber-physical systems, such as robots have some underlying DS properties associated with it, e.g., convergence, invariance to a set, etc. In this paper, a model learning method is developed such that the trajectories of the DS are invariant in a given compact set. Such invariant DS models can be used to generate trajectories of the robot that will always remain in a prescribed set. In order to achieve invariance to a set, Barrier certificates are employed. The DS is approximated using Extreme Learning Machine (ELM), and a parameter learning problem subject to Barrier certificates enforced at all the points in the prescribed set is solved. To solve an infinite constraint problem for enforcing Barrier Certificates at every point in a given compact set, a modified constraint is developed that is sufficient to hold the Barrier certificates in the entire set. An active sampling strategy is formulated to minimize the number of constraints in learning. Simulation results of ELM learning with and without Barrier certificates are presented which show the invariance property being preserved in the ELM learning when learning procedure involves Barrier constraints. The method is validated using experiments conducted on a robot arm recreating invariant trajectories inside a prescribed set.
keywords: {cyber-physical systems;feedforward neural nets;function approximation;Gaussian processes;learning (artificial intelligence);manipulators;nonlinear dynamical systems;optimisation;robot programming;dynamical system model;robot learning applications;cyber-physical systems;model learning method;ELM learning;invariance property;invariant trajectories;barrier certificates;parameter learning problem;active sampling based safe identification;extreme learning machines;infinite constraint problem;robot arm;barrier constraints;Robots;Trajectory;Safety;Stability analysis;Heuristic algorithms;Neurons;Convergence},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793891&isnumber=8793254

S. McLeod and J. Xiao, "Navigating Dynamically Unknown Environments Leveraging Past Experience," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 29-35.
doi: 10.1109/ICRA.2019.8793565
Abstract: To enable autonomous robot navigation among unknown dynamic obstacles, a real-time adaptive motion planner (RAMP) plans the robot motion online based on sensing the environment as the robot moves with sensors mounted on the robot. However, the sensed environmental data from the robot's local view is usually incomplete due to occlusions from obstacles and limited sensing range.This paper incorporates learning about the environment into the RAMP framework by leveraging the Hilbert Maps framework to generate a probabilistic model of occupancy of the unknown dynamic environment based on past observations. Utilizing this probabilistic model enables RAMP to reason about trajectory fitness when sensing information is partial and incomplete. This allows the RAMP robot to take advantage of what it has experienced from being in the dynamic environment before to inform its subsequent executions even though the dynamic environment changes in unknown ways. The effectiveness of incorporating such learned probabilistic data into RAMP is shown in both simulation and real experiments.
keywords: {adaptive control;collision avoidance;mobile robots;navigation;autonomous robot navigation;unknown dynamic obstacles;real-time adaptive motion planner;robot motion online;sensed environmental data;limited sensing range;RAMP framework;probabilistic model;unknown dynamic environment;sensing information;RAMP robot;dynamic environment changes;unknown ways;learned probabilistic data;Hilbert maps framework;dynamically unknown environment navigation;Robot sensing systems;Trajectory;Sociology;Statistics;Planning},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793565&isnumber=8793254

I. Arnekvist, D. Kragic and J. A. Stork, "VPE: Variational Policy Embedding for Transfer Reinforcement Learning," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 36-42.
doi: 10.1109/ICRA.2019.8793556
Abstract: Reinforcement Learning methods are capable of solving complex problems, but resulting policies might perform poorly in environments that are even slightly different. In robotics especially, training and deployment conditions often vary and data collection is expensive, making retraining undesirable. Simulation training allows for feasible training times, but on the other hand suffer from a reality-gap when applied in real-world settings. This raises the need of efficient adaptation of policies acting in new environments.We consider the problem of transferring knowledge within a family of similar Markov decision processes. We assume that Q-functions are generated by some low-dimensional latent variable. Given such a Q-function, we can find a master policy that can adapt given different values of this latent variable. Our method learns both the generative mapping and an approximate posterior of the latent variables, enabling identification of policies for new tasks by searching only in the latent space, rather than the space of all policies. The low-dimensional space, and master policy found by our method enables policies to quickly adapt to new environments. We demonstrate the method on both a pendulum swing-up task in simulation, and for simulation-to-real transfer on a pushing task.
keywords: {learning (artificial intelligence);Markov processes;pendulums;variational techniques;variational policy embedding;transfer reinforcement Learning;complex problems;deployment conditions;data collection;simulation training;Q-function;master policy;latent variables;latent space;low-dimensional space;simulation-to-real transfer;reinforcement learning methods;Markov decision processes;Optimization;Training;Task analysis;Robots;Adaptation models;Reinforcement learning;Supervised learning},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793556&isnumber=8793254

W. Kim, M. Tanaka, M. Okutomi and Y. Sasaki, "Automatic Labeled LiDAR Data Generation based on Precise Human Model," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 43-49.
doi: 10.1109/ICRA.2019.8793916
Abstract: Following improvements in deep neural networks, state-of-the-art networks have been proposed for human recognition using point clouds captured by LiDAR. However, the performance of these networks strongly depends on the training data. An issue with collecting training data is labeling. Labeling by humans is necessary to obtain the ground truth label; however, labeling requires huge costs. Therefore, we propose an automatic labeled data generation pipeline, for which we can change any parameters or data generation environments. Our approach uses a human model named Dhaiba and a background of Miraikan and consequently generated realistic artificial data. We present 500k + data generated by the proposed pipeline. This paper also describes the specification of the pipeline and data details with evaluations of various approaches.
keywords: {data analysis;image motion analysis;image recognition;learning (artificial intelligence);neural nets;optical radar;human recognition;point clouds;ground truth label;automatic labeled data generation pipeline;data generation environments;realistic artificial data;automatic labeled LiDAR data generation;precise human model;deep neural networks;Laser radar;Data models;Three-dimensional displays;Pipelines;Labeling;Legged locomotion;Solid modeling},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793916&isnumber=8793254

M. Siam et al., "Video Object Segmentation using Teacher-Student Adaptation in a Human Robot Interaction (HRI) Setting," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 50-56.
doi: 10.1109/ICRA.2019.8794254
Abstract: Video object segmentation is an essential task in robot manipulation to facilitate grasping and learning affordances. Incremental learning is important for robotics in unstructured environments. Inspired by the children learning process, human robot interaction (HRI) can be utilized to teach robots about the world guided by humans similar to how children learn from a parent or a teacher. A human teacher can show potential objects of interest to the robot, which is able to self adapt to the teaching signal without providing manual segmentation labels. We propose a novel teacher-student learning paradigm to teach robots about their surrounding environment. A two-stream motion and appearance “teacher” network provides pseudo-labels to adapt an appearance “student” network. The student network is able to segment the newly learned objects in other scenes, whether they are static or in motion. We also introduce a carefully designed dataset that serves the proposed HRI setup, denoted as (I)nteractive (V)ideo (O)bject (S)egmentation. Our IVOS dataset contains teaching videos of different objects, and manipulation tasks. Our proposed adaptation method outperforms the state-of-theart on DAVIS and FBMS with 6.8% and 1.2% in F-measure respectively. It improves over the baseline on IVOS dataset with 46.1% and 25.9% in mIoU.
keywords: {computer aided instruction;human-robot interaction;image segmentation;learning (artificial intelligence);manipulators;teaching;video signal processing;teacher-student learning paradigm;interactive video object segmentation;grasping affordances;children learning process;IVOS dataset;teaching signal;human teacher;unstructured environments;robotics;incremental learning;learning affordances;robot manipulation;human robot interaction setting;teacher-student adaptation;adaptation method;manipulation tasks;HRI setup;appearance student network;appearance teacher network;Task analysis;Adaptation models;Motion segmentation;Education;Object segmentation;Benchmark testing},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794254&isnumber=8793254

S. Funabashi, G. Yan, A. Geier, A. Schmitz, T. Ogata and S. Sugano, "Morphology-Specific Convolutional Neural Networks for Tactile Object Recognition with a Multi-Fingered Hand," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 57-63.
doi: 10.1109/ICRA.2019.8793901
Abstract: Distributed tactile sensors on multi-fingered hands can provide high-dimensional information for grasping objects, but it is not clear how to optimally process such abundant tactile information. The current paper explores the possibility of using a morphology-specific convolutional neural network (MS-CNN). uSkin tactile sensors are mounted on an Allegro Hand, which provides 720 force measurements (15 patches of uSkin modules with 16 triaxial force sensors each) in addition to 16 joint angle measurements. Consecutive layers in the CNN get input from parts of one finger segment, one finger, and the whole hand. Since the sensors give 3D (x, y, z) vector tactile information, inputs with 3 channels (x, y and z) are used in the first layer, based on the idea of such inputs for RGB images from cameras. Overall, the layers are combined, resulting in the building of a tactile map based on the relative position of the tactile sensors on the hand. Seven different combination variations were evaluated, and an over-95% object recognition rate with 20 objects was achieved, even though only one random time instance from a repeated squeezing motion of an object in an unknown pose within the hand was used as input.
keywords: {convolutional neural nets;dexterous manipulators;force measurement;force sensors;object recognition;tactile sensors;morphology-specific convolutional neural network;distributed tactile sensors;multifingered hands;high-dimensional information;grasping objects;abundant tactile information;MS-CNN;Allegro Hand;uSkin modules;consecutive layers;finger segment;tactile map;force measurements;joint angle measurements;object recognition rate;triaxial force sensors;Tactile sensors;Convolution;Task analysis;Object recognition},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793901&isnumber=8793254

A. Schaefer, J. Vertens, D. Büscher and W. Burgard, "A Maximum Likelihood Approach to Extract Finite Planes from 3-D Laser Scans," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 72-78.
doi: 10.1109/ICRA.2019.8794318
Abstract: Whether it is object detection, model reconstruction, laser odometry, or point cloud registration: Plane extraction is a vital component of many robotic systems. In this paper, we propose a strictly probabilistic method to detect finite planes in organized 3-D laser range scans. An agglomerative hierarchical clustering technique, our algorithm builds planes from bottom up, always extending a plane by the point that decreases the measurement likelihood of the scan the least. In contrast to most related methods, which rely on heuristics like orthogonal point-to-plane distance, we leverage the ray path information to compute the measurement likelihood. We evaluate our approach not only on the popular SegComp benchmark, but also provide a challenging synthetic dataset that overcomes SegComp's deficiencies. Both our implementation and the suggested dataset are available at [1].
keywords: {feature extraction;image registration;image segmentation;laser ranging;maximum likelihood estimation;pattern clustering;ray tracing;stereo image processing;measurement likelihood;point-to-plane distance;ray path information;maximum likelihood approach;object detection;model reconstruction;laser odometry;point cloud registration;robotic systems;strictly probabilistic method;agglomerative hierarchical clustering;3-D laser range scans;finite plane extraction;image segmentation;Laser modes;Three-dimensional displays;Measurement by laser beam;Clustering algorithms;Probabilistic logic;Computational modeling;Maximum likelihood estimation},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794318&isnumber=8793254

M. Lechner, R. Hasani, M. Zimmer, T. A. Henzinger and R. Grosu, "Designing Worm-inspired Neural Networks for Interpretable Robotic Control," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 87-94.
doi: 10.1109/ICRA.2019.8793840
Abstract: In this paper, we design novel liquid time-constant recurrent neural networks for robotic control, inspired by the brain of the nematode, C. elegans. In the worm's nervous system, neurons communicate through nonlinear time-varying synaptic links established amongst them by their particular wiring structure. This property enables neurons to express liquid time-constants dynamics and therefore allows the network to originate complex behaviors with a small number of neurons. We identify neuron-pair communication motifs as design operators and use them to configure compact neuronal network structures to govern sequential robotic tasks. The networks are systematically designed to map the environmental observations to motor actions, by their hierarchical topology from sensory neurons, through recurrently-wired interneurons, to motor neurons. The networks are then parametrized in a supervised-learning scheme by a search-based algorithm. We demonstrate that obtained networks realize interpretable dynamics. We evaluate their performance in controlling mobile and arm robots, and compare their attributes to other artificial neural network-based control agents. Finally, we experimentally show their superior resilience to environmental noise, compared to the existing machine learning-based methods.
keywords: {brain;manipulator dynamics;mobile robots;neurocontrollers;neurophysiology;nonlinear control systems;recurrent neural nets;search problems;supervised learning;time-varying systems;interpretable robotic control;nonlinear time-varying synaptic links;liquid time-constants dynamics;neuron-pair communication motifs;compact neuronal network structures;sequential robotic tasks;sensory neurons;recurrently-wired interneurons;motor neurons;interpretable dynamics;mobile arm robots;artificial neural network-based control agents;wiring structure;worm-inspired neural networks;liquid time-constant recurrent neural networks;nematode;C. elegans;supervised-learning scheme;search-based algorithm;Neurons;Biological neural networks;Robot sensing systems;Synapses;Correlation;Couplings},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793840&isnumber=8793254

Z. Tu, F. Fei, J. Zhang and X. Deng, "Acting Is Seeing: Navigating Tight Space Using Flapping Wings," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 95-101.
doi: 10.1109/ICRA.2019.8794084
Abstract: Wings of flying animals can not only generate lift and control torques but also can sense their surroundings. Such dual functions of sensing and actuation coupled in one element are particularly useful for small sized bio-inspired robotic flyers, whose weight, size, and power are under stringent constraint. In this work, we present the first flapping-wing robot using its flapping wings for environmental perception and navigation in tight space, without the need for any visual feedback. As the test platform, we introduce the Purdu Hummingbird, a flapping-wing robot with 17cm wingspan and 12 grams weight, with a pair of 30-40Hz flapping wings driven by only two actuators. By interpreting the wing loading feedback and its variations, the vehicle can detect the presence of environmental changes such as grounds, walls, stairs, obstacles and wind gust. The instantaneous wing loading can be obtained through the measurements and interpretation of the current feedback by the motors that actuate the wings. The effectiveness of the proposed approach is experimentally demonstrated on several challenging flight tasks without vision: terrain following, wall following and going through a narrow corridor. To ensure flight stability, a robust controller was designed for handling unforeseen disturbances during the flight. Sensing and navigating one's environment through actuator loading is a promising method for mobile robots, and it can serve as an alternative or complementary method to visual perception.
keywords: {aerospace robotics;biomimetics;control system synthesis;feedback;mobile robots;navigation;path planning;robust control;torque control;flapping-wing robot;wing loading feedback;instantaneous wing loading;bio-inspired robotic flyers;torque control;tight space navigation;Purdu Hummingbird;flight stability;robust controller design;Robot sensing systems;DC motors;Navigation;Loading;Aerodynamics},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794084&isnumber=8793254

Y. Wang, C. Frazelle, R. Sirohi, L. Li, I. D. Walker and K. E. Green, "Design and Characterization of a Novel Robotic Surface for Application to Compressed Physical Environments *," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 102-108.
doi: 10.1109/ICRA.2019.8794043
Abstract: Developments of robot arms are countless, but there has been little focus on robot surfaces for the reshaping of a habitable space - especially compliant surfaces. In this paper we introduce a novel, tendon-driven, robot surface comprised of aggregated, overlapping panels organized in a herringbone pattern. The individual 3D-printed panels and their behavior as an aggregation are inspired by the form and behavior of a pinecone. This paper presents our concept, design, and realization of this robot, and compares our prototype to simulations of four physical configurations that are formally distinct and suggestive of how the surface might be applied to habitable, physical space in response to human needs and wants. For the four configurations studied, we found a validating match between prototype and simulations. The paper concludes with a consideration of potential applications for robot surfaces like this one.
keywords: {biomechanics;design engineering;mobile robots;compressed physical environments;robot arms;robot surface;compliant surfaces;habitable space;physical space;tendon-driven robotic surface;herringbone pattern;3D-printed panels;Prototypes;Service robots;Springs;Surface treatment;Robot kinematics;Surface waves},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794043&isnumber=8793254

F. Fei, Z. Tu, J. Zhang and X. Deng, "Learning Extreme Hummingbird Maneuvers on Flapping Wing Robots," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 109-115.
doi: 10.1109/ICRA.2019.8794100
Abstract: Biological studies show that hummingbirds can perform extreme aerobatic maneuvers during fast escape. Given a sudden looming visual stimulus at hover, a hummingbird initiates a fast backward translation coupled with a 180-degree yaw turn, which is followed by instant posture stabilization in just under 10 wingbeats. Consider the wingbeat frequency of 40Hz, this aggressive maneuver is carried out in just 0.2 seconds. Inspired by the hummingbirds' near-maximal performance during such extreme maneuvers, we developed a flight control strategy and experimentally demonstrated that such maneuverability can be achieved by an at-scale 12-gram hummingbird robot equipped with just two actuators driving a pair of flapping wings up to 40Hz. The proposed hybrid control policy combines model-based nonlinear control with model-free reinforcement learning. We used the model-based nonlinear control for nominal flight conditions where dynamic models are relatively accurate. During extreme maneuvers when the modeling error becomes unmanageable, we use a model-free reinforcement learning policy trained and optimized in simulation to 'destabilize' the system for peak performance during maneuvering. The hybrid policy manifests a maneuver that is close to that observed in hummingbirds. Direct simulation-to-real transfer is achieved, demonstrating the hummingbird-like fast evasive maneuvers on the at-scale hummingbird robot.
keywords: {aerodynamics;aerospace components;aerospace robotics;aircraft control;control engineering computing;learning (artificial intelligence);mobile robots;nonlinear control systems;position control;robot dynamics;robot kinematics;stability;extreme aerobatic maneuvers;visual stimulus;180-degree yaw turn;wingbeat frequency;flight control strategy;hybrid control policy;model-based nonlinear control;model-free reinforcement learning policy;hummingbird-like fast evasive maneuvers;extreme hummingbird maneuvers;flapping wing robots;backward translation;posture stabilization;hummingbird robot;frequency 40.0 Hz;time 0.2 s;Aerodynamics;Vehicle dynamics;Uncertainty;Robots;Adaptation models;Torque;Actuators},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794100&isnumber=8793254

F. Tang, H. Li and Y. Wu, "FMD Stereo SLAM: Fusing MVG and Direct Formulation Towards Accurate and Fast Stereo SLAM," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 133-139.
doi: 10.1109/ICRA.2019.8793664
Abstract: We propose a novel stereo visual SLAM framework considering both accuracy and speed at the same time. The framework makes full use of the advantages of key-feature-based multiple view geometry (MVG) and direct-based formulation. At the front-end, our system performs direct formulation and constant motion model to predict a robust initial pose, reprojects local map to find 3D-2D correspondence and finally refines pose by the reprojection error minimization. This frontend process makes our system faster. At the back-end, MVG is used to estimate 3D structure. When a new keyframe is inserted, new mappoints are generated by triangulating. In order to improve the accuracy of the proposed system, bad mappoints are removed and a global map is kept by bundle adjustment. Especially, the stereo constraint is performed to optimize the map. This back-end process makes our system more accurate. Experimental evaluation on EuRoC dataset shows that the proposed algorithm can run at more than 100 frames per second on a consumer computer while achieving highly competitive accuracy.
keywords: {feature extraction;motion estimation;pose estimation;SLAM (robots);stereo image processing;key-feature-based multiple view geometry;global map;3D structure;bundle adjustment;fast stereo SLAM;direct formulation;local map;constant motion model;direct-based formulation;novel stereo visual SLAM framework;FMD stereo SLAM;back-end process;stereo constraint;reprojection error minimization;Simultaneous localization and mapping;Cameras;Three-dimensional displays;Feature extraction;Visualization;Robot vision systems;Two dimensional displays},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793664&isnumber=8793254

S. Schreiberhuber, J. Prankl, T. Patten and M. Vincze, "ScalableFusion: High-resolution Mesh-based Real-time 3D Reconstruction," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 140-146.
doi: 10.1109/ICRA.2019.8793654
Abstract: Dense 3D reconstructions generate globally consistent data of the environment suitable for many robot applications. Current RGB-D based reconstructions, however, only maintain the color resolution equal to the depth resolution of the used sensor. This firmly limits the precision and realism of the generated reconstructions. In this paper we present a real-time approach for creating and maintaining a surface reconstruction in as high as possible geometrical fidelity with full sensor resolution for its colorization (or surface texture). A multi-scale memory management process and a Level of Detail scheme enable equally detailed reconstructions to be generated at small scales, such as objects, as well as large scales, such as rooms or buildings. We showcase the benefit of this novel pipeline with a PrimeSense RGB-D camera as well as combining the depth channel of this camera with a high resolution global shutter camera. Further experiments show that our memory management approach allows us to scale up to larger domains that are not achievable with current state-of-the-art methods.
keywords: {cameras;image colour analysis;image reconstruction;image resolution;image sensors;SLAM (robots);sensor resolution;colorization;multiscale memory management process;high resolution global shutter camera;memory management approach;high-resolution mesh-based real-time 3D reconstruction;dense 3D reconstructions;robot applications;color resolution;depth resolution;surface reconstruction;surface texture;geometrical fidelity;RGB-D based reconstructions;PrimeSense RGB-D camera;Surface reconstruction;Image color analysis;Robot sensing systems;Geometry;Three-dimensional displays;Surface texture;Image reconstruction},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793654&isnumber=8793254

P. Chakravarty, P. Narayanan and T. Roussel, "GEN-SLAM: Generative Modeling for Monocular Simultaneous Localization and Mapping," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 147-153.
doi: 10.1109/ICRA.2019.8793530
Abstract: We present a Deep Learning based system for the twin tasks of localization and obstacle avoidance essential to any mobile robot. Our system learns from conventional geometric SLAM, and outputs, using a single camera, the topological pose of the camera in an environment, and the depth map of obstacles around it. We use a CNN to localize in a topological map, and a conditional VAE to output depth for a camera image, conditional on this topological location estimation. We demonstrate the effectiveness of our monocular localization and depth estimation system on simulated and real datasets.
keywords: {cameras;collision avoidance;convolutional neural nets;learning (artificial intelligence);mobile robots;pose estimation;robot vision;SLAM (robots);depth estimation system;GEN-SLAM;generative modeling;Deep Learning based system;obstacle avoidance;mobile robot;conventional geometric SLAM;single camera;topological map;camera image;topological location estimation;monocular localization;monocular simultaneous localization and mapping;Cameras;Image reconstruction;Simultaneous localization and mapping;Decoding;Training},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793530&isnumber=8793254

F. Schenk and F. Fraundorfer, "RESLAM: A real-time robust edge-based SLAM system," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 154-160.
doi: 10.1109/ICRA.2019.8794462
Abstract: Simultaneous Localization and Mapping is a key requirement for many practical applications in robotics. In this work, we present RESLAM, a novel edge-based SLAM system for RGBD sensors. Due to their sparse representation, larger convergence basin and stability under illumination changes, edges are a promising alternative to feature-based or other direct approaches. We build a complete SLAM pipeline with camera pose estimation, sliding window optimization, loop closure and relocalisation capabilities that utilizes edges throughout all steps. In our system, we additionally refine the initial depth from the sensor, the camera poses and the camera intrinsics in a sliding window to increase accuracy. Further, we introduce an edge-based verification for loop closures that can also be applied for relocalisation. We evaluate RESLAM on wide variety of benchmark datasets that include difficult scenes and camera motions and also present qualitative results. We show that this novel edge-based SLAM system performs comparable to state-of-the-art methods, while running in real-time on a CPU. RESLAM is available as open-source software1.
keywords: {cameras;edge detection;image colour analysis;image representation;image sensors;motion estimation;optimisation;pose estimation;robot vision;SLAM (robots);camera intrinsics;sliding window;edge-based verification;RESLAM;camera motions;RGBD sensors;sparse representation;SLAM pipeline;robust edge-based SLAM system;simultaneous localization and mapping;Image edge detection;Cameras;Simultaneous localization and mapping;Optimization;Real-time systems;Microsoft Windows},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794462&isnumber=8793254

Y. Chen, S. Huang, R. Fitch, L. Zhao, H. Yu and D. Yang, "On-line 3D active pose-graph SLAM based on key poses using graph topology and sub-maps," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 169-175.
doi: 10.1109/ICRA.2019.8793632
Abstract: In this paper, we present an on-line active pose-graph simultaneous localization and mapping (SLAM) frame-work for robots in three-dimensional (3D) environments using graph topology and sub-maps. This framework aims to find the best trajectory for loop-closure by re-visiting old poses based on the T-optimality and D-optimality metrics of the Fisher information matrix (FIM) in pose-graph SLAM. In order to reduce computational complexity, graph topologies are introduced, including weighted node degree (T-optimality metric) and weighted tree-connectivity (D-optimality metric), to choose a candidate trajectory and several key poses. With the help of the key poses, a sampling-based path planning method and a continuous-time trajectory optimization method are combined hierarchically and applied in the whole framework. So as to further improve the real-time capability of the method, the sub-map joining method is used in the estimation and planning process for large-scale active SLAM problems. In simulations and experiments, we validate our approach by comparing against existing methods, and we demonstrate the on-line planning part using a quad-rotor unmanned aerial vehicle (UAV).
keywords: {autonomous aerial vehicles;computational complexity;graph theory;mobile robots;optimisation;path planning;remotely operated vehicles;robot vision;SLAM (robots);graph topology;pose-graph simultaneous localization;three-dimensional environments;D-optimality metrics;weighted node degree;T-optimality metric;sampling-based path;continuous-time trajectory optimization method;large-scale active SLAM problems;submap joining method;online 3D active pose-graph SLAM;Simultaneous localization and mapping;Measurement;Trajectory;Planning;Three-dimensional displays;Uncertainty},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793632&isnumber=8793254

P. S. Schmitt, F. Wirnshofer, K. M. Wurm, G. v. Wichert and W. Burgard, "Modeling and Planning Manipulation in Dynamic Environments," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 176-182.
doi: 10.1109/ICRA.2019.8793824
Abstract: In this paper we propose a new model for sequential manipulation tasks that also considers robot dynamics and time-variant environments. From this model we automatically derive constraint-based controllers and use them as steering functions in a kinodynamic manipulation planner. The resulting plan is not a trajectory but a sequence of controllers that react online to disturbances. We validated our approach in simulation and on a real robot. In the experiments our approach plans and executes dual-robot manipulation tasks with online collision avoidance and reactions to estimates of object poses.
keywords: {collision avoidance;manipulator dynamics;mobile robots;pose estimation;robot vision;steering systems;kinodynamic manipulation planner;dynamic environments;robot dynamics;time-variant environments;manipulation modeling;manipulation planning;online collision avoidance;object pose estimation;steering functions;Planning;Task analysis;Collision avoidance;Manipulator dynamics;Grippers},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793824&isnumber=8793254

J. Lee, Y. Cho, C. Nam, J. Park and C. Kim, "Efficient Obstacle Rearrangement for Object Manipulation Tasks in Cluttered Environments," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 183-189.
doi: 10.1109/ICRA.2019.8793616
Abstract: We present an algorithm that produces a plan for relocating obstacles in order to grasp a target in clutter by a robotic manipulator without collisions. We consider configurations where objects are densely populated in a constrained and confined space. Thus, there exists no collision-free path for the manipulator without relocating obstacles. Since the problem of planning for object rearrangement has shown to be NP-hard, it is difficult to perform manipulation tasks efficiently which could frequently happen in service domains (e.g., taking out a target from a shelf or a fridge). Our proposed planner employs a collision avoidance scheme which has been widely used in mobile robot navigation. The planner determines an obstacle to be removed quickly in real time. It also can deal with dynamic changes in the configuration (e.g., changes in object poses). Our method is shown to be complete and runs in polynomial time. Experimental results in a realistic simulated environment show that our method improves up to 31% of the execution time compared to other competitors.
keywords: {collision avoidance;computational complexity;manipulators;mobile robots;navigation;object manipulation tasks;cluttered environments;robotic manipulator;constrained confined space;collision-free path;object rearrangement;NP-hard;service domains;collision avoidance scheme;mobile robot navigation;object poses;obstacle rearrangement;polynomial time;Histograms;Planning;Task analysis;Grasping;End effectors},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793616&isnumber=8793254

M. Görner, R. Haschke, H. Ritter and J. Zhang, "MoveIt! Task Constructor for Task-Level Motion Planning," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 190-196.
doi: 10.1109/ICRA.2019.8793898
Abstract: A lot of motion planning research in robotics focuses on efficient means to find trajectories between individual start and goal regions, but it remains challenging to specify and plan robotic manipulation actions which consist of multiple interdependent subtasks. The Task Constructor framework we present in this work provides a flexible and transparent way to define and plan such actions, enhancing the capabilities of the popular robotic manipulation framework MoveIt!.11The Task Constructor framework is publicly available at https://github.com/ros-planning/moveit_task_constructor Subproblems are solved in isolation in black-box planning stages and a common interface is used to pass solution hypotheses between stages. The framework enables the hierarchical organization of basic stages using containers, allowing for sequential as well as parallel compositions. The flexibility of the framework is illustrated in multiple scenarios performed on various robot platforms, including bimanual ones.
keywords: {manipulators;path planning;robotic manipulation actions;Task Constructor framework;black-box planning stages;task-level motion planning;robotic manipulation framework;MoveIt!;Planning;Task analysis;Robots;Trajectory;Containers;Generators;Collision avoidance},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793898&isnumber=8793254

P. Mohammadi, D. Kubus and J. J. Steil, "Exploiting Environment Contacts of Serial Manipulators," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 197-203.
doi: 10.1109/ICRA.2019.8794027
Abstract: We explore the characteristics of secondary contacts when applying forces with the end-effector of a robot and address the question when these secondary contacts can increase maximum applicable end-effector forces or reduce required actuator efforts. To this end, we formalize the effect of such secondary contacts in terms of required actuator efforts and derive efficiency bounds depending on the contact characteristics and robot configuration. Our findings are confirmed by experiments with a redundant serial manipulator.
keywords: {actuators;end effectors;force control;manipulators;environment contacts;serial manipulators;redundant serial manipulator;robot configuration;end-effector forces;actuator;Actuators;Force;Friction;Kinematics;End effectors},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794027&isnumber=8793254

P. Long, T. Keleştemur, A. Ö. Önol and T. Padir, "optimization-Based Human-in-the-Loop Manipulation Using Joint Space Polytopes," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 204-210.
doi: 10.1109/ICRA.2019.8794071
Abstract: This paper presents a new method of maximizing the free space for a robot operating in a constrained environment under operator supervision. The objective is to make the resulting trajectories more robust to operator commands and/or changes in the environment. To represent the volume of free space, the constrained manipulability polytopes are used. These polytopes embed the distance to obstacles, the distance to joint limits and the distance to singular configurations. The volume of the resulting Cartesian polyhedron is used in an optimization-based motion planner to create the trajectories. Additionally, we show how fast collision-free inverse kinematic solutions can be obtained by exploiting the pre-computed inequality constraints. The proposed algorithm is validated in simulation and experimentally.
keywords: {collision avoidance;geometry;manipulator kinematics;mobile robots;optimisation;motion planner;human-in-the-loop manipulation;optimization;robot operation;Cartesian polyhedron;fast collision-free inverse kinematic;joint space polytopes;singular configurations;constrained manipulability polytopes;operator commands;End effectors;Trajectory;Kinematics;Task analysis;Collision avoidance;Aerospace electronics},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794071&isnumber=8793254

E. Huang, Z. Jia and M. T. Mason, "Large-Scale Multi-Object Rearrangement," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 211-218.
doi: 10.1109/ICRA.2019.8793946
Abstract: This paper describes a new robotic tabletop rearrangement system, and presents experimental results. The tasks involve rearranging as many as 30 to 100 blocks, sometimes packed with a density of up to 40%. The high packing factor forces the system to push several objects at a time, making accurate simulation difficult, if not impossible. Nonetheless, the system achieves goals specifying the pose of every object, with an average precision of ± 1 mm and ± 2°. The system searches through policy rollouts of simulated pushing actions, using an Iterated Local Search technique to escape local minima. In real world execution, the system executes just one action from a policy, then uses a vision system to update the estimated task state, and replans. The system accepts a fully general description of task goals, which means it can solve the singulation and separation problems addressed in prior work, but can also solve sorting problems and spell out words, among other things. The paper includes examples of several solved problems, statistical analysis of the system's behavior on different types of problems, and some discussion of limitations, insights, and future work.
keywords: {iterative methods;optimisation;robot vision;search problems;robotic tabletop rearrangement system;high packing factor forces;simulated pushing actions;vision system;iterated local search technique;large-scale multiobject rearrangement;Planning;Task analysis;Grasping;Robots;Annealing;Trajectory;Markov processes},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793946&isnumber=8793254

L. Dai, Z. Ge, N. Jiao, J. Shi and L. Liu, "Manipulation Using Microrobot Driven by Optothermally Generated Surface Bubble," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 219-224.
doi: 10.1109/ICRA.2019.8794242
Abstract: A manipulation technique based on optothermally generated surface bubbles is proposed in this paper. The manipulation and assembly of microstructures are completed by using bubbles. In addition, the hydrogel microstructures are also used as microrobots driven by the bubble to operate and pattern the microspheres. Considering that many materials and lasers with different wavelength have been used for generating bubbles by optothermal effects, absorptivity and transmissivity are used as indicators of selections. Besides, the size of the bubble can be controlled by the frequency and time of the laser. This technique is supposed to be applied for manipulation of cells, microparticles and microstructures.
keywords: {bioMEMS;bubbles;hydrogels;microrobots;photothermal effects;optothermal effects;microparticles;optothermally generated surface bubble;manipulation technique;hydrogel microstructures;microrobots;absorptivity;transmissivity;Microstructure;Silicon;Gold;Surface waves;Laser theory;Photothermal effects},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794242&isnumber=8793254

M. Kaynak, F. Ayhan and M. S. Sakar, "Compound micromachines powered by acoustic streaming," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 225-230.
doi: 10.1109/ICRA.2019.8793481
Abstract: This paper presents the design, fabrication, and operation of compound micromachines powered by acoustic streaming. The machine components were directly incorporated around pillars serving as shafts without further assembly steps using a single-step in situ polymerization process controlled by a programmable projector. Two strategies were presented for harvesting acoustic energy using sharp-edged structures. The first method is based on on-board pumping of fluids and the second method involves engineering of rotors. The implementation of these strategies resulted in the construction of microscale turbines and engines that can be coupled to gear trains for adaptable transmission of mechanical power. We provide a number of further improvements that may together lead to development of compact yet powerful robotic manipulation systems inside microfluidic devices.
keywords: {acoustic streaming;energy harvesting;gears;machine tools;manipulators;microfabrication;microfluidics;micromachining;polymerisation;pumps;in-situ polymerization process;acoustic energy harvesting;robotic manipulation systems;microfluidic devices;microfluidic devices;mechanical power;microscale turbines;programmable projector;assembly steps;machine components;acoustic streaming;compound micromachines;Acoustics;Rotors;Bars;Oscillators;Turbines;Microchannels},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793481&isnumber=8793254

R. Zhang, A. Sherehiy, Z. Yang, D. Wei, C. K. Harnett and D. O. Popa, "ChevBot – An Untethered Microrobot Powered by Laser for Microfactory Applications," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 231-236.
doi: 10.1109/ICRA.2019.8793856
Abstract: In this paper, we introduce a new class of submillimeter robot (ChevBot) for microfactory applications in dry environments, powered by a 532 nm laser beam. ChevBot is an untethered microrobot propelled by a thermal Micro Electro Mechanical (MEMS) actuator upon exposure to the laser light. Novel models for opto-thermal-mechanical energy conversion are proposed to describe the microrobot's locomotion mechanism. First, an opto-thermal simulation model is presented which is experimentally validated with static displacement measurements with microrobots tethered to the substrate. Then, stick and slip motion of the microrobot was predicted using a dynamic extension of our simulation model, and experiments were conducted to validate this model in one dimension. Promising microrobot designs were fabricated on a silicon on insulator (SOI) wafer with 20 μm device layer and a dimple was assembled at the bottom to initiate directional locomotion on a silicon substrate. Validation experiments demonstrate that exposure to laser power below 2W and repetition frequencies below 60 kHz can generate actuator displacements of a few microns, and 46 μm/s locomotion velocity.
keywords: {displacement measurement;industrial robots;integrated circuit manufacture;laser beam applications;microactuators;micromechanical devices;microrobots;semiconductor technology;silicon-on-insulator;ChevBot;microfactory applications;dry environments;thermal MicroElectro Mechanical actuator;laser light;opto-thermal-mechanical energy conversion;opto-thermal simulation model;static displacement measurements;dynamic extension;directional locomotion;laser power;actuator displacements;locomotion velocity;submillimeter robot;laser beam;untethered microrobot;microrobot designs;silicon on insulator wafer;Laser modes;Actuators;Laser beams;Measurement by laser beam;Power lasers;Silicon;Predictive models},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793856&isnumber=8793254

Y. Lin, X. Liu and T. Arai, "Capillary Ionic Transistor and Precise Transport Control for Nano Manipulation," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 237-242.
doi: 10.1109/ICRA.2019.8793755
Abstract: Capillary Ionic Transistor (CIT) is introduced as a nanodevice which provides control of ionic transport through nanochannel by gate voltage. CIT is Ionic transistor which employs pulled capillary as nanochannel with tip diameter smaller than 100 nm. We observed that gate voltage applied to gate electrode, deposited on the outer wall of capillary, affect a conductance of nanochannel, due to change of surface charge at the solution/capillary interface. Negative gate voltage corresponds to lower conductivity and positive gate increase conductance of the channel. This effect strongly depends on the size of the channel. In general, at least one dimension of the channel has to be small enough for electrical double layer to overlap. As a demonstration of the gate control ability, we performed Si nanoparticle delivery via CIT and recorded the deliverance through resistive pulse method. Size and velocity measurement are also conducted, to showcase the versatility of CIT device.
keywords: {capillarity;electrodes;elemental semiconductors;nanoparticles;silicon;size measurement;surface charging;transistors;velocity measurement;negative gate voltage;positive gate;nanodevice;surface charge;capillary interface;solution interface;electrical double layer;nanoparticle delivery;resistive pulse method;velocity measurement;size measurement;CIT device;gate control ability;gate electrode;nanochannel;ionic transport;CIT;Capillary Ionic Transistor;precise transport control;Si;Logic gates;Electrodes;Transistors;Nanobioscience;Electric potential;Ions;Nanoscale devices;nanofluidic;gate voltage control;sub 100nm;micromanipulator},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793755&isnumber=8793254

K. Murotani, K. Yamamoto, T. Ko and Y. Nakamura, "Resolved Viscoelasticity Control Considering Singularity for Knee-stretched Walking of a Humanoid," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 250-255.
doi: 10.1109/ICRA.2019.8793605
Abstract: This paper describes a stable knee-stretched walking of a humanoid by the resolved viscoelasticity control (RVC). The RVC method resolves multiple viscoelasticities in task-space, including the center of mass viscoelasticity for balancing, into joint-space viscoelasticity. Although a robust and compliant motion was achieved by the RVC method in previous studies, the conventional knee-bent posture to avoid the kinematic singularity suffered large knee joint torque. In this study, we propose an extension of the RVC capable of the kinematic singularity. We demonstrate through simulations and experiments that the RVC method considering the singularity achieves a stable and human-like walking, reducing the knee joint torque and improving the energy efficiency.
keywords: {humanoid robots;legged locomotion;motion control;viscoelasticity;task-space;mass viscoelasticity;joint-space viscoelasticity;robust motion;compliant motion;RVC method;kinematic singularity;knee joint torque;RVC capable;humanoid;stable knee-stretched walking;knee-bent posture;resolved viscoelasticity control;Task analysis;Knee;Legged locomotion;Kinematics;Trajectory;Foot;Humanoid robots},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793605&isnumber=8793254

J. Ding, C. Zhou, Z. Guo, X. Xiao and N. Tsagarakis, "Versatile Reactive Bipedal Locomotion Planning Through Hierarchical Optimization," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 256-262.
doi: 10.1109/ICRA.2019.8794072
Abstract: When experiencing disturbances during locomotion, human beings use several strategies to maintain balance, e.g. changing posture, modulating step frequency and location. However, when it comes to the gait generation for humanoid robots, modifying step time or body posture in real time introduces nonlinearities in the walking dynamics, thus increases the complexity of the planning. In this paper, we propose a two-layer hierarchical optimization framework to address this issue and provide the humanoids with the abilities of step time and step location adjustment, Center of Mass (CoM) height variation and angular momentum adaptation. In the first layer, times and locations of consecutive two steps are modulated online based on the current CoM state using the Linear Inverted Pendulum Model. By introducing new optimization variables to substitute the hyperbolic functions of step time, the derivatives of the objective function and feasibility constraints are analytically derived, thus reduces the computational cost. Then, taking the generated horizontal CoM trajectory, step times and step locations as inputs, CoM height and angular momentum changes are optimized by the second layer nonlinear model predictive control. This whole procedure will be repeated until the termination condition is met. The improved recovery capability under external disturbances is validated in simulation studies.
keywords: {control nonlinearities;humanoid robots;legged locomotion;linear systems;motion control;nonlinear control systems;optimisation;path planning;pendulums;predictive control;robot dynamics;step frequency;humanoid robots;hierarchical optimization;angular momentum;nonlinear model predictive control;reactive bipedal locomotion planning;nonlinearities;walking dynamics;step time abilities;step location adjustment;Center of Mass height variation;linear inverted pendulum model;robot gait generation;Legged locomotion;Optimization;Trajectory;Humanoid robots;Linear programming;Dynamics},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794072&isnumber=8793254

T. Li, H. Geyer, C. G. Atkeson and A. Rai, "Using Deep Reinforcement Learning to Learn High-Level Policies on the ATRIAS Biped," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 263-269.
doi: 10.1109/ICRA.2019.8793864
Abstract: Learning controllers for bipedal robots is a challenging problem, often requiring expert knowledge and extensive tuning of parameters that vary in different situations. Recently, deep reinforcement learning has shown promise at automatically learning controllers for complex systems in simulation. This has been followed by a push towards learning controllers that can be transferred between simulation and hardware, primarily with the use of domain randomization. However, domain randomization can make the problem of finding stable controllers even more challenging, especially for under actuated bipedal robots. In this work, we explore whether policies learned in simulation can be transferred to hardware with the use of high-fidelity simulators and structured controllers. We learn a neural network policy which is a part of a more structured controller. While the neural network is learned in simulation, the rest of the controller stays fixed, and can be tuned by the expert as needed. We show that using this approach can greatly speed up the rate of learning in simulation, as well as enable transfer of policies between simulation and hardware. We present our results on an ATRIAS robot and explore the effect of action spaces and cost functions on the rate of transfer between simulation and hardware. Our results show that structured policies can indeed be learned in simulation and implemented on hardware successfully. This has several advantages, as the structure preserves the intuitive nature of the policy, and the neural network improves the performance of the hand-designed policy. In this way, we propose a way of using neural networks to improve expert designed controllers, while maintaining ease of understanding.
keywords: {control system synthesis;learning (artificial intelligence);legged locomotion;neurocontrollers;deep reinforcement learning;expert knowledge;domain randomization;stable controllers;high-fidelity simulators;neural network policy;ATRIAS robot;bipedal robots;Neural networks;Hardware;Legged locomotion;Reinforcement learning;Torso;Foot},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793864&isnumber=8793254

S. Piperakis, S. Timotheatos and P. Trahanias, "Unsupervised Gait Phase Estimation for Humanoid Robot Walking*," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 270-276.
doi: 10.1109/ICRA.2019.8793598
Abstract: Contact detection is an important topic in contemporary humanoid robotic research. Up to date control and state estimation schemes readily assume that feet contact status is known in advance. In this work, we elaborate on a broader question: in which gait phase is the robot currently in? We introduce an unsupervised learning framework for gait phase estimation based solely on proprioceptive sensing, namely joint encoder, inertial measurement unit and force/torque data. Initially, a meaningful physical explanation on data acquisition is presented. Subsequently, dimensionality reduction is performed to obtain a compact low-dimensional feature representation followed by clustering into three groups, one for each gait phase. The proposed framework is qualitatively and quantitatively assessed in simulation with ground-truth data of uneven/rough terrain walking gaits and insights about the latent gait phase dynamics are drawn. Additionally, its efficacy and robustness is demonstrated when incorporated in leg odometry computation. Since our implementation is based on sensing that is commonly available on humanoids today, we release an open-source ROS/Python package to reinforce further research endeavors.
keywords: {data acquisition;data reduction;humanoid robots;legged locomotion;pattern clustering;phase estimation;robot dynamics;robust control;state estimation;unsupervised learning;unsupervised gait phase estimation;humanoid robot walking;contact detection;feet contact status;proprioceptive sensing;inertial measurement unit;data acquisition;dimensionality reduction;state estimation;unsupervised learning;feature representation;gait phase dynamics;joint encoder;force data;torque data;clustering;robustness;legged robots;Legged locomotion;Humanoid robots;Robot sensing systems;Kinematics;Unsupervised learning},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793598&isnumber=8793254

S. Caron, A. Kheddar and O. Tempier, "Stair Climbing Stabilization of the HRP-4 Humanoid Robot using Whole-body Admittance Control," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 277-283.
doi: 10.1109/ICRA.2019.8794348
Abstract: We consider dynamic stair climbing with the HRP-4 humanoid robot as part of an Airbus manufacturing use-case demonstrator. We share experimental knowledge gathered so as to achieve this task, which HRP-4 had never been challenged to before. In particular, we extend walking stabilization based on linear inverted pendulum tracking [1] by quadratic programming-based wrench distribution and a whole-body admittance controller that applies both end-effector and CoM strategies. While existing stabilizers tend to use either one or the other, our experience suggests that the combination of these two approaches improves tracking performance. We demonstrate this solution in an on-site experiment where HRP4 climbs an industrial staircase with 18.5 cm high steps, and release our walking controller as open source software.
keywords: {end effectors;humanoid robots;legged locomotion;linear systems;nonlinear control systems;pendulums;quadratic programming;robot dynamics;stability;stair climbing stabilization;HRP-4 humanoid robot;whole-body admittance control;Airbus manufacturing use-case demonstrator;quadratic programming-based wrench distribution;whole-body admittance controller;walking controller;dynamic stair climbing;walking stabilization;linear inverted pendulum tracking;end-effector;CoM strategy;tracking performance;industrial staircase;open source software;size 18.5 cm;Admittance;Legged locomotion;Foot;Humanoid robots;Force;Task analysis},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794348&isnumber=8793254

G. A. Castillo, B. Weng, A. Hereid, Z. Wang and W. Zhang, "Reinforcement Learning Meets Hybrid Zero Dynamics: A Case Study for RABBIT," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 284-290.
doi: 10.1109/ICRA.2019.8793627
Abstract: The design of feedback controllers for bipedal robots is challenging due to the hybrid nature of its dynamics and the complexity imposed by high-dimensional bipedal models. In this paper, we present a novel approach for the design of feedback controllers using Reinforcement Learning (RL) and Hybrid Zero Dynamics (HZD). Existing RL approaches for bipedal walking are inefficient as they do not consider the underlying physics, often requires substantial training, and the resulting controller may not be applicable to real robots. HZD is a powerful tool for bipedal control with local stability guarantees of the walking limit cycles. In this paper, we propose a non traditional RL structure that embeds the HZD framework into the policy learning. More specifically, we propose to use RL to find a control policy that maps from the robot's reduced order states to a set of parameters that define the desired trajectories for the robot's joints through the virtual constraints. Then, these trajectories are tracked using an adaptive PD controller. The method results in a stable and robust control policy that is able to track variable speed within a continuous interval. Robustness of the policy is evaluated by applying external forces to the torso of the robot. The proposed RL framework is implemented and demonstrated in OpenAI Gym with the MuJoCo physics engine based on the well-known RABBIT robot model.
keywords: {adaptive control;control engineering computing;control system synthesis;feedback;learning (artificial intelligence);legged locomotion;mobile robots;PD control;robust control;stability;feedback controllers;bipedal robots;high-dimensional bipedal models;bipedal walking;bipedal control;walking limit cycles;HZD framework;policy learning;adaptive PD controller;stable control policy;robust control policy;RL framework;RABBIT robot model;reinforcement learning;local stability;hybrid zero dynamics;OpenAI gym;MuJoCo physics engine;Legged locomotion;Hip;Rabbits;Robot kinematics;Training;Computational modeling},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793627&isnumber=8793254

M. BROSSARD and S. BONNABEL, "Learning Wheel Odometry and IMU Errors for Localization," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 291-297.
doi: 10.1109/ICRA.2019.8794237
Abstract: Odometry techniques are key to autonomous robot navigation, since they enable self-localization in the environment. However, designing a robust odometry system is particularly challenging when camera and LiDAR are uninformative or unavailable. In this paper, we leverage recent advances in deep learning and variational inference to correct dynamical and observation models for state-space systems. The methodology trains Gaussian processes on the residual between the original model and the ground truth, and is applied on publicly available datasets for robot navigation based on two wheel encoders, a fiber optic gyro, and an Inertial Measurement Unit (IMU). We also propose to build an Extended Kalman Filter (EKF) on the learned model using wheel speed sensors and the fiber optic gyro for state propagation, and the IMU to update the estimated state. Experimental results clearly demonstrate that the (learned) corrected models and EKF are more accurate than their original counterparts.
keywords: {cameras;distance measurement;Gaussian processes;image filtering;Kalman filters;learning (artificial intelligence);mobile robots;nonlinear filters;path planning;robot vision;wheel odometry;IMU errors;odometry techniques;autonomous robot navigation;robust odometry system;camera;deep learning;variational inference;observation models;state-space systems;Gaussian processes;ground truth;wheel encoders;fiber optic gyro;EKF;wheel speed sensors;inertial measurement unit;extended Kalman filter;Wheels;Kernel;Mobile robots;Sensors;Gaussian processes;Training;Gaussian process;odometry estimation;variational inference;Kalman filter},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794237&isnumber=8793254

S. H. Cen and P. Newman, "Radar-only ego-motion estimation in difficult settings via graph matching," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 298-304.
doi: 10.1109/ICRA.2019.8793990
Abstract: Radar detects stable, long-range objects under variable weather and lighting conditions, making it a reliable and versatile sensor well suited for ego-motion estimation. In this work, we propose a radar-only odometry pipeline that is highly robust to radar artifacts (e.g., speckle noise and false positives) and requires only one input parameter. We demonstrate its ability to adapt across diverse settings, from urban UK to off-road Iceland, achieving a scan matching accuracy of approximately 5.20 cm and 0.0929 deg when using GPS as ground truth (compared to visual odometry's 5.77 cm and 0.1032 deg). We present algorithms for key point extraction and data association, framing the latter as a graph matching optimization problem, and provide an in-depth system analysis.
keywords: {distance measurement;feature extraction;Global Positioning System;graph theory;image matching;motion estimation;object detection;radar detection;sensor fusion;speckle noise;scan matching accuracy;visual odometry;ego-motion estimation;stable range objects;long-range objects;variable weather;lighting conditions;radar-only odometry pipeline;radar artifacts;key point extraction;data association;graph matching optimization problem;Robot sensing systems;Azimuth;Radar measurements;Radar detection;Feature extraction;Estimation},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793990&isnumber=8793254

G. D. Arana, O. A. Hafez, M. Joerger and M. Spenko, "Recursive Integrity Monitoring for Mobile Robot Localization Safety," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 305-311.
doi: 10.1109/ICRA.2019.8794115
Abstract: This paper presents a new methodology to quantify robot localization safety by evaluating integrity risk, a performance metric widely used in open-sky aviation applications that has been recently extended to mobile ground robots. Here, a robot is localized by feeding relative measurements to mapped landmarks into an Extended Kalman Filter while a sequence of innovations is evaluated for fault detection. The main contribution is the derivation of a sequential chi-squared integrity monitoring methodology that maintains constant computation requirements by employing a preceding time window and, at the same time, is robust against faults occurring prior to the window. Additionally, no assumptions are made on either the nature or shape of the faults because safety is evaluated under the worst possible combination of sensor faults.
keywords: {fault diagnosis;Kalman filters;mobile robots;nonlinear filters;constant computation requirements;sequential chi-squared integrity monitoring methodology;fault detection;Extended Kalman Filter;mobile ground robots;open-sky aviation applications;integrity risk;mobile robot localization safety;recursive integrity monitoring;preceding time window;Monitoring;Feature extraction;Safety;Fault detection;Robot sensing systems;Kalman filters},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794115&isnumber=8793254

A. Welte, P. Xu and P. Bonnifait, "Four-Wheeled Dead-Reckoning Model Calibration using RTS Smoothing," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 312-318.
doi: 10.1109/ICRA.2019.8794270
Abstract: Localization is one of the main challenges to be addressed to develop autonomous vehicles able to perform complex maneuvers on roads opened to public traffic. Having an accurate dead-reckoning system is an essential step to reach this objective. This paper presents a dead-reckoning model for car-like vehicles that performs the data fusion of complementary and redundant sensors: wheel encoders, yaw rate gyro and steering wheel measurements. In order to get an accurate dead-reckoning system with a drift reduced to the minimum, the parameters have to be well calibrated and the procedure has to be simple and efficient. We present a method able to accurately calibrate the parameters without knowing the ground truth by using a Rauch-Tung-Striebel smoothing scheme which enables to obtain state estimates as close to the ground truth as possible. The smoothed estimates are then used within a optimization process to calibrate the model parameters. The method has been tested using data recorded from an experimental vehicle on public roads. The results show a significant diminution of the dead-reckoning drift compared to a commonly used calibration method. We evaluate finally the average distance a vehicle can navigate without exteroceptive sensors by using the proposed four-wheeled dead reckoning system.
keywords: {automobiles;calibration;inertial navigation;Kalman filters;mobile robots;navigation;path planning;remotely operated vehicles;sensor fusion;smoothing methods;state estimation;RTS smoothing;autonomous vehicles;accurate dead-reckoning system;car-like vehicles;complementary sensors;redundant sensors;wheel encoders;yaw rate gyro;steering wheel measurements;ground truth;smoothing scheme;smoothed estimates;model parameters;experimental vehicle;public roads;dead-reckoning drift;commonly used calibration method;dead reckoning system;four-wheeled dead-reckoning model calibration;complex maneuvers;public traffic;Wheels;Sensors;Smoothing methods;Global navigation satellite system;Calibration;Dead reckoning;Radio frequency},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794270&isnumber=8793254

P. Yin et al., "A Multi-Domain Feature Learning Method for Visual Place Recognition," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 319-324.
doi: 10.1109/ICRA.2019.8793752
Abstract: Visual Place Recognition (VPR) is an important component in both computer vision and robotics applications, thanks to its ability to determine whether a place has been visited and where specifically. A major challenge in VPR is to handle changes of environmental conditions including weather, season and illumination. Most VPR methods try to improve the place recognition performance by ignoring the environmental factors, leading to decreased accuracy decreases when environmental conditions change significantly, such as day versus night. To this end, we propose an end-to-end conditional visual place recognition method. Specifically, we introduce the multi-domain feature learning method (MDFL) to capture multiple attribute-descriptions for a given place, and then use a feature detaching module to separate the environmental condition-related features from those that are not. The only label required within this feature learning pipeline is the environmental condition. Evaluation of the proposed method is conducted on the multi-season NORDLAND dataset, and the multi-weather GTAV dataset. Experimental results show that our method improves the feature robustness against variant environmental conditions.
keywords: {feature extraction;image recognition;learning (artificial intelligence);mobile robots;object recognition;robot vision;computer vision;robotics applications;VPR methods;place recognition performance;environmental factors;end-to-end conditional visual place recognition method;multidomain feature learning method;feature detaching module;environmental condition-related features;feature learning pipeline;multiseason NORDLAND dataset;multiweather GTAV dataset;feature robustness;variant environmental conditions;Feature extraction;Entropy;Visualization;Task analysis;Decoding;Robots;Upper bound},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793752&isnumber=8793254

S. Bryner, G. Gallego, H. Rebecq and D. Scaramuzza, "Event-based, Direct Camera Tracking from a Photometric 3D Map using Nonlinear Optimization," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 325-331.
doi: 10.1109/ICRA.2019.8794255
Abstract: Event cameras are novel bio-inspired vision sensors that output pixel-level intensity changes, called “events”, instead of traditional video images. These asynchronous sensors naturally respond to motion in the scene with very low latency (microseconds) and have a very high dynamic range. These features, along with a very low power consumption, make event cameras an ideal sensor for fast robot localization and wearable applications, such as AR/VR and gaming. Considering these applications, we present a method to track the 6-DOF pose of an event camera in a known environment, which we contemplate to be described by a photometric 3D map (i.e., intensity plus depth information) built via classic dense 3D reconstruction algorithms. Our approach uses the raw events, directly, without intermediate features, within a maximum-likelihood framework to estimate the camera motion that best explains the events via a generative model. We successfully evaluate the method using both simulated and real data, and show improved results over the state of the art. We release the datasets to the public to foster reproducibility and research in this topic.
keywords: {cameras;image reconstruction;image sensors;maximum likelihood estimation;motion estimation;motion measurement;nonlinear programming;photometry;pose estimation;asynchronous sensors;low power consumption;photometric 3D map;classic dense 3D reconstruction algorithms;bioinspired vision sensors;video imaging;output pixel-level intensity;event-based direct camera tracking;nonlinear optimization;robot localization;AR-VR;6-DOF pose tracking;maximum-likelihood framework;event camera motion estimation;Cameras;Robot vision systems;Three-dimensional displays;Optimization},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794255&isnumber=8793254

H. Kawano, "Linear Heterogeneous Reconfiguration of Cubic Modular Robots via Simultaneous Tunneling and Permutation," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 332-338.
doi: 10.1109/ICRA.2019.8793594
Abstract: Reconfiguring heterogeneous modular robots in which all modules are not identical is much more time consuming than reconfiguring homogeneous ones, because ordinary heterogeneous reconfiguration is a combination of homogeneous transformation and heterogeneous permutation. While linear homogeneous transformation has been accomplished in previous research, linear heterogeneous permutation has not. This paper studies a reconfiguration algorithm for heterogeneous lattice modular robots with linear operation time cost. The algorithm is based on simultaneous tunneling and permutation, where a robot transforms its configuration via tunneling motion while permutation of each module's position is performed simultaneously during the tunneling transformation. To achieve this, we introduce the idea of a transparent meta-module that allows modules belonging to a meta-module to pass through the spaces occupied by other meta-modules. We prove the correctness and completeness of the proposed algorithm for a 2×2×2 cubic meta-module-based connected robot structure. We also show examples of the reconfiguration simulations of heterogeneous modular robots by the proposed algorithm.
keywords: {computational complexity;control engineering computing;distributed control;evolutionary computation;mobile robots;multi-robot systems;reconfigurable architectures;heterogeneous lattice modular robots;linear operation time cost;2×2×2 cubic meta-module-based connected robot structure;heterogeneous modular robots;linear heterogeneous reconfiguration;cubic modular robots;ordinary heterogeneous reconfiguration;linear homogeneous transformation;linear heterogeneous permutation;simultaneous tunneling and permutation;Tunneling;Computer aided software engineering;Cameras;Robot vision systems;Robot kinematics;Lattices},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793594&isnumber=8793254

N. Melenbrink and J. Werfel, "Autonomous Sheet Pile Driving Robots for Soil Stabilization," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 339-345.
doi: 10.1109/ICRA.2019.8793546
Abstract: Soil stabilization is a fundamental component of nearly all construction projects, ranging from commercial construction to environmental restoration projects. Previous work in autonomous construction has generally not considered these essential stabilization and anchoring tasks. In this work we present Romu, an autonomous robot capable of building continuous linear structures by using a vibratory hammer to drive interlocking sheet piles into soil. We report on hardware parameters and their effects on pile driving performance, and demonstrate autonomous operation in both controlled and natural environments. Finally, we present simulations in which a small swarm of robots build with sheet piles in example terrains, or apply an alternate spray-based stabilizing agent, and quantify the ability of each intervention to mitigate hydraulic erosion.
keywords: {construction equipment;construction industry;erosion;foundations;geotechnical engineering;hammers (machines);mobile robots;soil;stability;autonomous sheet pile;soil stabilization;construction projects;environmental restoration projects;autonomous robot;continuous linear structures;vibratory hammer;hardware parameters;spray-based stabilizing agent;hydraulic erosion;Robots;Soil;Task analysis;Dams;Force;Actuators;Automation},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793546&isnumber=8793254

G. Li, B. Gabrich, D. Saldaña, J. Das, V. Kumar and M. Yim, "ModQuad-Vi: A Vision-Based Self-Assembling Modular Quadrotor," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 346-352.
doi: 10.1109/ICRA.2019.8794056
Abstract: Flying modular robots have the potential to rapidly form temporary structures. In the literature, docking actions rely on external systems and indoor infrastructures for relative pose estimation. In contrast to related work, we provide local estimation during the self-assembly process to avoid dependency on external systems. In this paper, we introduce ModQuad-Vi, a flying modular robot that is aimed to operate in outdoor environments. We propose a new robot design and vision-based docking method. Our design is based on a quadrotor platform with onboard computation and visual perception. Our control method is able to accurately align modules for docking actions. Additionally, we present the dynamics and a geometric controller for the aerial modular system. Experiments validate the vision-based docking method with successful results.
keywords: {autonomous aerial vehicles;indoor radio;mobile robots;path planning;pose estimation;robot vision;self-assembly;relative pose estimation;local estimation;self-assembly process;external systems;ModQuad-Vi;flying modular robot;robot design;vision-based docking method;docking actions;aerial modular system;vision-based self-assembling modular quadrotor;temporary structures;indoor infrastructures;Robot kinematics;Acceleration;Rotors;Force;Trajectory;Navigation},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794056&isnumber=8793254

D. Lee, M. Hwang and D. Kwon, "Robotic endoscopy system (easyEndo) with a robotic arm mountable on a conventional endoscope," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 367-372.
doi: 10.1109/ICRA.2019.8793626
Abstract: The use of flexible endoscope has been rising inconveniences. Steering of the distal section is not intuitive and the weight of the endoscope burdens a physical pressure on physicians who use it continuously for a long time. Also, the limited dexterity of an instrument makes therapeutic procedures more difficult, and further the unintended communications often occur during cooperation with assistants. These degrade the efficiency and thus increase the procedure time. In this paper, we propose a robotic endoscopy system (easyEndo) that can be mounted on a conventional endoscope and facilitate solo-endoscopy with two intuitive hand-held controllers. Furthermore, a robotic arm is presented that can be attached to the endoscope to assist with tissue traction. To validate the robotic endoscopy system, experiments to simulate biopsy and lesion marking were conducted with novices. The results showed that the robotic manipulations improved efficiency and reduced workload than manual manipulation. Subsequently, a prototype of the robotic arm was attached at the distal end of the endoscope, and the feasibility of tissue traction was confirmed by a simulation of pulling a rubber band.
keywords: {biological tissues;biomechanics;endoscopes;manipulators;medical robotics;tissue traction;lesion;rubber band;robotic arm;flexible endoscope;robotic manipulations;intuitive hand-held controllers;solo-endoscopy;conventional endoscope;robotic endoscopy system;Endoscopes;Manipulators;Instruments;Medical services;Cameras;Gears},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793626&isnumber=8793254

S. Kwon et al., "Design and Fabrication of Transformable Head Structures for Endoscopic Catheters*," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 373-378.
doi: 10.1109/ICRA.2019.8794256
Abstract: We present a transformable catheter head structure for endoscopic catheter allowing the simultaneous use of a camera module and a large tool channel introduced through a small incision. At the site of interest, the head with a camera can be expanded from the initial straight configuration, which opens a window for advancing a tool that is located behind the camera. Two different designs were proposed and prototyped. One option has flexure joints directly fabricated at the distal end of a polymer catheter by laser micro-machining, while another design employs a hinged metal head assembled at the tip of the same type of catheter. The kinematic behavior of each head was evaluated during the head-up and tip steering motions, and compared with each other to draw a selection guideline between them. Experimental results prove the feasibility of the proposed head structure for smarter endoscopic catheters.
keywords: {biomedical optical imaging;cameras;catheters;endoscopes;medical image processing;medical robotics;polymers;camera module;transformable catheter head structure;endoscopic catheter;laser micromachining;polymer catheter;Catheters;Head;Magnetic heads;Tools;Electron tubes;Polymers;Cameras},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794256&isnumber=8793254

A. Schmitz, S. Treratanakulchai, P. Berthet-Rayne and G. Yang, "A Rolling-Tip Flexible Instrument for Minimally Invasive Surgery," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 379-385.
doi: 10.1109/ICRA.2019.8793480
Abstract: Snake-like robots are commonly used in Minimally Invasive Surgery as they are able to reach areas deep inside the human body. These robots have instruments that are deployed out of the robot's head and controlled via tendons, which connect the instrument to motors at the proximal end. In most currently available systems the instruments are lacking a rolling motion of the end-effector.In this paper, we present a new instrument prototype for a snake-like robot that can perform a stable in-place rolling motion. The prototype has a diameter of 4mm, uses 13 tendons and has 6 degrees of freedom. The robot can bend and roll to high angles, and strongly improves the dexterity compared to an instrument without rolling capabilities. In the evaluation we show that the rolling-tip gripper can rotate about 165° and is capable of applying forces up to 6.5N.
keywords: {dexterous manipulators;end effectors;flexible manipulators;grippers;medical robotics;mobile robots;prototypes;surgery;rolling-tip flexible instrument;human body;end-effector;instrument prototype;in-place rolling motion;rolling-tip gripper;minimally invasive surgery;snake-like robots;tendons;6 degrees-of-freedom;dexterity;Tendons;Instruments;Grippers;Fasteners;Surgery;End effectors},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793480&isnumber=8793254

G. Ma, W. A. Ross, I. Hill, N. Narasimhan and P. J. Codd, "A Novel Laser Scalpel System for Computer-assisted Laser Surgery," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 386-392.
doi: 10.1109/ICRA.2019.8794066
Abstract: Laser scalpels are utilized across a variety of surgical and dermatological procedures due to their precision and non-contact nature. This paper presents a novel laser scalpel system for superficial laser therapy applications. The system integrates a RGB-D camera, a 3D triangulation sensor and a carbon dioxide (CO2) laser scalpel for computer-assisted laser surgery. To accurately ablate targets chosen from the color image, a 3D extrinsic calibration method between the RGB-D camera frame and the laser coordinate system is implemented. The accuracy of the calibration method is tested on phantoms with planar and cylindrical surfaces. Positive error and negative error, as defined as undershooting and overshooting over the target area, are reported for each test. For 60 total test cases, the root-mean-square of the positive and negative error in both planar and cylindrical phantoms is less than 1.0 mm, with a maximum absolute error less than 2.0 mm. This work demonstrates the feasibility of automated laser therapy with surgeon oversight via our sensor system.
keywords: {biomedical optical imaging;cameras;laser applications in medicine;medical computing;phantoms;radiation therapy;skin;surgery;planar phantoms;cylindrical phantoms;root-mean-square;carbon dioxide laser scalpel;laser scalpel system;3D extrinsic calibration method;3D triangulation sensor;superficial laser therapy applications;dermatological procedures;surgical procedures;computer-assisted laser surgery;sensor system;automated laser therapy;laser coordinate system;RGB-D camera frame;Three-dimensional displays;Lasers;Cameras;Calibration;Surgery;Measurement by laser beam;Color},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794066&isnumber=8793254

M. Bowman, S. Li and X. Zhang, "Intent-Uncertainty-Aware Grasp Planning for Robust Robot Assistance in Telemanipulation," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 409-415.
doi: 10.1109/ICRA.2019.8793819
Abstract: Promoting a robot agent's autonomy level, which allows it to understand the human operator's intent and provide motion assistance to achieve it, has demonstrated great advantages to the operator's intent in teleoperation. However, the research has been limited to the target approaching process. We advance the shared control technique one step further to deal with the more challenging object manipulation task. Appropriately manipulating an object is challenging as it requires fine motion constraints for a certain manipulation task. Although these motion constraints are critical for task success, they are subtle to observe from ambiguous human motion. The disembodiment problem and physical discrepancy between the human and robot hands bring additional uncertainty, make the object manipulation task more challenging. Moreover, there is a lack of modeling and planning techniques that can effectively combine the human motion input and robot agent's motion input while accounting for the ambiguity of the human intent. To overcome this challenge, we built a multi-task robot grasping model and developed an intent-uncertainty-aware grasp planner to generate robust grasp poses given the ambiguous human intent inference inputs. With this validated modeling and planning techniques, it is expected to extend teleoperated robots' functionality and adoption in practical telemanipulation scenarios.
keywords: {dexterous manipulators;grippers;mobile robots;motion control;path planning;telerobotics;intent-uncertainty-aware grasp planning;robust robot assistance;robot agent;motion assistance;target approaching process;fine motion constraints;ambiguous human motion;robot hands;planning techniques;human motion input;multitask robot;intent-uncertainty-aware grasp planner;robust grasp;ambiguous human intent inference inputs;teleoperated robots;object manipulation task;Task analysis;Robots;Planning;Uncertainty;Handover;Grasping},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793819&isnumber=8793254

S. Li et al., "Vision-based Teleoperation of Shadow Dexterous Hand using End-to-End Deep Neural Network," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 416-422.
doi: 10.1109/ICRA.2019.8794277
Abstract: In this paper, we present TeachNet, a novel neural network architecture for intuitive and markerless vision-based teleoperation of dexterous robotic hands. Robot joint angles are directly generated from depth images of the human hand that produce visually similar robot hand poses in an end-to-end fashion. The special structure of TeachNet, combined with a consistency loss function, handles the differences in appearance and anatomy between human and robotic hands. A synchronized human-robot training set is generated from an existing dataset of labeled depth images of the human hand and simulated depth images of a robotic hand. The final training set includes 400K pairwise depth images and joint angles of a Shadow C6 robotic hand. The network evaluation results verify the superiority of TeachNet, especially regarding the high-precision condition. Imitation experiments and grasp tasks teleoperated by novice users demonstrate that TeachNet is more reliable and faster than the state-of-the-art vision-based teleoperation method.
keywords: {dexterous manipulators;human-robot interaction;image classification;learning (artificial intelligence);neurocontrollers;pose estimation;robot vision;telerobotics;TeachNet;Shadow dexterous hand;end-to-end deep neural network;intuitive vision-based teleoperation;markerless vision-based teleoperation;dexterous robotic hands;robot joint angles;human hand;visually similar robot hand;consistency loss function;human hands;human-robot training set;labeled depth images;simulated depth images;Shadow C6 robotic hand;pairwise depth images;vision-based teleoperation method;Training;Pose estimation;Three-dimensional displays;Neural networks;Robot sensing systems},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794277&isnumber=8793254

M. Minelli, F. Ferraguti, N. Piccinelli, R. Muradore and C. Secchi, "An energy-shared two-layer approach for multi-master-multi-slave bilateral teleoperation systems," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 423-429.
doi: 10.1109/ICRA.2019.8794335
Abstract: In this paper, a two-layer architecture for the bilateral teleoperation of multi-arms systems with communication delay is presented. We extend the single-master-single-slave two layer approach proposed in [1] by connecting multiple robots to a single energy tank. This allows to minimize the conservativeness due to passivity preservation and to increment the level of transparency that can be achieved. The proposed approach is implemented on a realistic surgical scenario developed within the EU-funded SARAS project.
keywords: {delay systems;medical robotics;surgery;telerobotics;energy-shared two-layer approach;multimaster-multislave bilateral teleoperation systems;two-layer architecture;multiarms systems;communication delay;energy tank;single-master-single-slave two layer approach;passivity preservation;surgical scenario;Computer architecture;Surgery;Robot kinematics;Force;Delays;Communication channels},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794335&isnumber=8793254

M. Selvaggio, P. Robuffo Giordano, F. Ficuciellol and B. Siciliano, "Passive Task-Prioritized Shared-Control Teleoperation with Haptic Guidance," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 430-436.
doi: 10.1109/ICRA.2019.8794197
Abstract: Robot teleoperation is widely used for several hazardous applications. To increase teleoperator capabilities shared-control methods can be employed. In this paper, we present a passive task-prioritized shared-control method for remote telemanipulation of redundant robots. The proposed method fuses the task-prioritized control architecture with haptic guidance techniques to realize a shared-control framework for teleoperation systems. To preserve the semi-autonomous telerobotic system safety, passivity is analyzed and an energy-tanks passivity-based controller is developed. The proposed theoretical results are validated through experiments involving a real haptic device and a simulated slave robot.
keywords: {control engineering computing;haptic interfaces;telerobotics;passive task-prioritized shared-control teleoperation;robot teleoperation;teleoperator capabilities shared-control methods;passive task-prioritized shared-control method;redundant robots;task-prioritized control architecture;haptic guidance techniques;shared-control framework;semiautonomous telerobotic system safety;energy-tanks passivity-based controller;simulated slave robot;Task analysis;Haptic interfaces;Jacobian matrices;Manipulators;Robot kinematics;Couplings},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794197&isnumber=8793254

D. V. Gealy et al., "Quasi-Direct Drive for Low-Cost Compliant Robotic Manipulation," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 437-443.
doi: 10.1109/ICRA.2019.8794236
Abstract: Robots must cost less and be force-controlled to enable widespread, safe deployment in unconstrained human environments. We propose Quasi-Direct Drive actuation as a capable paradigm for robotic force-controlled manipulation in human environments at low-cost. Our prototype - Blue - is a human scale 7 Degree of Freedom arm with 2kg payload. Blue can cost less than $5000. We show that Blue has dynamic properties that meet or exceed the needs of human operators: the robot has a nominal position-control bandwidth of 7.5Hz and repeatability within 4mm. We demonstrate a Virtual Reality based interface that can be used as a method for telepresence and collecting robot training demonstrations. Manufacturability, scaling, and potential use-cases for the Blue system are also addressed. Videos and additional information can be found online at berkeleyopenarms.github.io.
keywords: {control engineering computing;force control;manipulators;position control;telerobotics;user interfaces;virtual reality;QuasiDirect Drive actuation;robotic force-controlled manipulation;telepresence;Blue system;human environments;robot training demonstrations;7 degree of freedom arm;position-control bandwidth;virtual reality based interface;compliant robotic manipulation;mass 2.0 kg;frequency 7.5 Hz;size 4.0 mm;Manipulators;Payloads;Bandwidth;Robot sensing systems;Task analysis;Belts},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794236&isnumber=8793254

F. Richter, Y. Zhang, Y. Zhi, R. K. Orosco and M. C. Yip, "Augmented Reality Predictive Displays to Help Mitigate the Effects of Delayed Telesurgery," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 444-450.
doi: 10.1109/ICRA.2019.8794051
Abstract: Surgical robots offer the exciting potential for remote telesurgery, but advances are needed to make this technology efficient and accurate to ensure patient safety. Achieving these goals is hindered by the deleterious effects of latency between the remote operator and the bedside robot. Predictive displays have found success in overcoming these effects by giving the operator immediate visual feedback. However, previously developed predictive displays can not be directly applied to telesurgery due to the unique challenges in tracking the 3D geometry of the surgical environment. In this paper, we present the first predictive display for teleoperated surgical robots. The predicted display is stereoscopic, utilizes Augmented Reality (AR) to show the predicted motions alongside the complex tissue found in-situ within surgical environments, and overcomes the challenges in accurately tracking slave-tools in real-time. We call this a Stereoscopic AR Predictive Display (SARPD). To test the SARPD's performance, we conducted a user study with ten participants on the da Vinci® Surgical System. The results showed with statistical significance that using SARPD decreased time to complete task while having no effect on error rates when operating under delay.
keywords: {augmented reality;medical computing;medical robotics;surgery;telemedicine;telerobotics;da Vinci surgical system;SARPD;augmented reality predictive displays;stereoscopic AR predictive display;predictive displays;visual feedback;teleoperated surgical robots;surgical environment;remote operator;remote telesurgery;Delays;Rendering (computer graphics);Cameras;Stereo image processing;Real-time systems;Transforms;Robots},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794051&isnumber=8793254

M. T. Leddy and A. M. Dollar, "Stability Optimization of Two-Fingered Anthropomorphic Hands for Precision Grasping with a Single Actuator," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 451-457.
doi: 10.1109/ICRA.2019.8793812
Abstract: In this paper, we present a constrained optimization framework for evaluating the post-contact stability of underactuated precision grasping configurations with a single degree of actuation. Relationships between key anthropomorphic design parameters including link length ratios, transmission ratios, joint stiffness ratios and palm width are developed with applications in upper limb prosthetic design. In addition to grasp stability, we examine post-contact system work, to reduce reconfiguration, and consider the range of objects that can be stably grasped. External wrenches were simulated on a subset of the heuristically evaluated optimal solutions and an optimal configuration was experimentally tested to determine favorable wrench resistible gripper orientations for grasp planning applications.
keywords: {actuators;biomechanics;dexterous manipulators;grippers;prosthetics;link length ratios;anthropomorphic design parameters;grasp planning applications;optimal configuration;heuristically evaluated optimal solutions;post-contact system work;upper limb prosthetic design;palm width;joint stiffness ratios;transmission ratios;post-contact stability;constrained optimization framework;single actuator;precision grasping;two-fingered anthropomorphic hands;stability optimization;Force;Grasping;Actuators;Tendons;Optimization;Stability criteria},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793812&isnumber=8793254

T. Miunske, C. Holzapfel, E. Baumgartner and H. Reuss, "A new Approach for an Adaptive Linear Quadratic Regulated Motion Cueing Algorithm for an 8 DoF Full Motion Driving Simulator," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 497-503.
doi: 10.1109/ICRA.2019.8794109
Abstract: In this contribution, a new adaptive motion cueing algorithm for a full motion driving simulator at the University of Stuttgart is presented, which allows kinematic vehicle movements to be taken into account. These are adequately processed via a state-flow chart and transferred to the motion cueing algorithm in such a way that the dynamic of the Stuttgart Driving Simulator can be used much more efficiently. Furthermore, a linear quadratic error minimization of the mentioned algorithm is presented. The primary objective is to provide a more realistic driving experience to the driver.
keywords: {linear quadratic control;minimisation;motion control;road vehicles;vehicle dynamics;8 DoF full motion driving simulator;Stuttgart Driving Simulator;state-flow chart;kinematic vehicle movements;motion driving simulator;adaptive motion cueing algorithm;adaptive linear quadratic regulated motion cueing algorithm;linear quadratic error minimization;Acceleration;Switches;Vehicles;Heuristic algorithms;Vehicle dynamics;Kinematics},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794109&isnumber=8793254

J. Merlet, "Singularity of Cable-Driven Parallel Robot With Sagging Cables: Preliminary Investigation," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 504-509.
doi: 10.1109/ICRA.2019.8794218
Abstract: This paper addresses for the first time the singularity analysis of cable-driven parallel robot (CDPR) with sagging cables using the Irvine model. We present the mathematical framework of singularity analysis of CDPR using this cable model. We then show that, besides a cable model representation singularity, both the inverse and forward kinematics (IK and FK) have a singularity type, called parallel robot singularity, which correspond to the singularity of an equivalent parallel robot with rigid legs. We then show that both the IK and FK have also full singularities, that are not parallel robot singularity and are obtained when two of the IK or FK solution branches intersect. IK singularity will usually lie on the border of the CDPR workspace. We then exhibit an algorithm that allow one to prove that a singularity exist in the neighborhood of a given pose and to estimate its location with an arbitrary accuracy. Examples are provided for parallel robot, IK and FK singularities. However we have not been able to determine examples of combined singularity where both the IK and FK are singular (besides parallel robot singularity).
keywords: {cables (mechanical);manipulator kinematics;cable-driven parallel robot;sagging cables;singularity analysis;CDPR;Irvine model;cable model representation singularity;singularity type;IK singularity;parallel robot singularity;inverse kinematics;forward kinematics;rigid legs;Parallel robots;Mathematical model;Kinematics;Zirconium;Legged locomotion;End effectors;parallel robot cable-driven parallel robot;singularity},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794218&isnumber=8793254

S. Fan, S. Fan, W. Lan and G. Song, "A defect identification approach of operations for the driving element of multi-duty parallel manipulators," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 510-516.
doi: 10.1109/ICRA.2019.8794326
Abstract: In order to improve the machining efficiency and the flexibility of manufacturing system, the study of multi-duty parallel manipulators has attracted the interest of some researchers. In this paper, according to the effects of different operations on the driving element, a demarcation diagram for distinguishing different duties, such as statics, low-speed but heavy-load, high-speed but low-load and high-speed but heavy-load, is proposed, and a defect identification approach to prevent the occurrence of defects for multi-duty parallel manipulators is presented. Taking the 1PU+3UPS parallel manipulator as an instance, an analysis method to the statics and dynamics is investigated by means of the screw theory and the proposed virtual screw. Based on the numerical example, the results show that the classification and practicability of operations can be accurately identified by the proposed demarcation diagram and defect identification approach, respectively.
keywords: {end effectors;fasteners;force control;industrial manipulators;machining;defect identification approach;driving element;multiduty parallel manipulators;heavy-load;1PU+3UPS parallel manipulator;machining efficiency improvement;Force;Indexes;Machining;Fasteners;Manipulator dynamics;Dynamics},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794326&isnumber=8793254

H. Sellet, I. Khayour, L. Cuvillon, S. Durand and J. Gangloff, "Active Damping of Parallel Robots Driven by Flexible Cables Using Cold-Gas Thrusters," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 530-536.
doi: 10.1109/ICRA.2019.8794061
Abstract: This work is a preliminary study assessing the feasibility of using cold-gas thrusters for active damping of flexible cable-driven parallel robots. The concept is validated experimentally on a planar robot embedding custom-built supersonic air thrusters operating at an industry-standard pressure level.
keywords: {cables (mechanical);damping;flexible manipulators;manipulator dynamics;supersonic flow;planar robot;custom-built supersonic air thrusters;active damping;cold-gas thrusters;flexible cable-driven parallel robots;industry-standard pressure level;Attitude control;Vibrations;End effectors;Valves;Damping;Bars},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794061&isnumber=8793254

M. Hamaya et al., "Exploiting Human and Robot Muscle Synergies for Human-in-the-loop Optimization of EMG-based Assistive Strategies," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 549-555.
doi: 10.1109/ICRA.2019.8794082
Abstract: In this study, we propose a novel human-in-the-loop optimization approach for exoskeleton robot control. We develop a method to optimize widely-used Electromyography (EMG)-based assistive strategies. If we use multiple EMG channels to control multi-DoF robots, optimization process becomes complex and requires a large amount of data. To make the optimization tractable, we exploit the synergies both of the human muscles and artificial muscles of the exoskeleton robots to reduce the number of parameters of the assistive strategies. We show that we can extract the synergies not only from the user's muscle activities but from pneumatic artificial muscle (PAMs) contractions of the exoskeleton robot. Then, we adopt a Bayesian optimization method to acquire the parameters for assisting human movements by iteratively identifying the user's preferences of the assistive strategies. We conducted experiments to evaluate our proposed method with a PAMs-driven upper-limb exoskeleton robot. Our method successfully learned assistive strategies from the human-in-theloop optimization with a practicable number of interactions.
keywords: {artificial limbs;biomechanics;electromyography;medical robotics;motion control;optimisation;robot muscle synergies;EMG-based assistive strategies;exoskeleton robot control;Electromyography-based assistive strategies;multiple EMG channels;multiDoF robots;optimization process;human muscles;pneumatic artificial muscle contractions;Bayesian optimization method;human movements;PAMs-driven upper-limb exoskeleton robot;human-in-theloop optimization;human-in-the-loop optimization approach;Muscles;Robots;Optimization;Exoskeletons;Electromyography;Bayes methods;Pneumatic systems},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794082&isnumber=8793254

J. Hunt and H. Lee, "Development of a Low Inertia Parallel Actuated Shoulder Exoskeleton Robot for the Characterization of Neuromuscular Property during Static Posture and Dynamic Movement," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 556-562.
doi: 10.1109/ICRA.2019.8794181
Abstract: The purpose of this work is to introduce a newly developed exoskeleton robot designed to characterize the neuromuscular properties of the shoulder, including intrinsic and reflexive mechanisms, during static posture and dynamic movement in a 3-dimensional space. Quantitative characterization of these properties requires fast perturbation (>100°/s) to separate their contribution from that of voluntary mechanism. Understanding these properties of the shoulder control could assist in the rehabilitation or enhancement of upper limb performance during physical human-robot interaction. The device can be described as a new type of spherical parallel manipulator (SPM) that utilizes three 4-bar (4B) substructures to decouple and control roll, pitch and yaw of the shoulder. By utilizing a parallel architecture, the 4BSPM exoskeleton has the advantage of high acceleration, fast enough to satisfy the speed requirement for the characterization of distinct neuromuscular properties of the shoulder. In this work, the prototype is presented, along with an evaluation of its position accuracy and step response. The development and preliminary testing of the 4B-SPM exoskeleton presented in this work demonstrates its potential to be a useful tool for studying the neuromuscular mechanisms of the shoulder joint.
keywords: {actuators;biomechanics;human-robot interaction;manipulator kinematics;medical robotics;patient rehabilitation;low inertia parallel actuated shoulder exoskeleton robot;neuromuscular property;dynamic movement;intrinsic mechanisms;reflexive mechanisms;voluntary mechanism;shoulder control;upper limb performance;physical human-robot interaction;spherical parallel manipulator;control roll;parallel architecture;speed requirement;4B-SPM exoskeleton;neuromuscular mechanisms;shoulder joint;neuromuscular properties;Shoulder;Exoskeletons;Servomotors;Robots;Neuromuscular;Prototypes;Impedance},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794181&isnumber=8793254

A. C. d. Oliveira, K. Warburton, J. S. Sulzer and A. D. Deshpande, "Effort Estimation in Robot-aided Training with a Neural Network," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 563-569.
doi: 10.1109/ICRA.2019.8794281
Abstract: Robotic exoskeletons open up promising interventions during post-stroke rehabilitation by assisting individuals with sensorimotor impairments to complete therapy tasks. These devices have the ability to provide variable assistance tailored to individual-specific needs and, additionally, can measure several parameters associated with the movement execution. Metrics representative of movement quality are important to guide individualized treatment. While robots can provide data with high resolution, robustness, and consistency, the delineation of the human contribution in the presence of the kinematic guidance introduced by the robotic assistance is a significant challenge. In this paper, we propose a method for assessing voluntary effort from an individual fitted in an upper-body exoskeleton called Harmony. The method separates the active torques generated by the wearer from the effects caused by unmodeled dynamics and passive neuromuscular properties and involuntary forces. Preliminary results show that the effort estimated using the proposed method is consistent with the effort associated with muscle activity and is also sensitive to different levels, indicating that it can reliably evaluate user's contribution to movement. This method has the potential to serve as a high resolution assessment tool to monitor progress of movement quality throughout the treatment and evaluate motor recovery.
keywords: {biomechanics;medical robotics;motion control;muscle;neurocontrollers;patient rehabilitation;patient treatment;robot dynamics;robot kinematics;robust control;torque control;wearable robots;effort estimation;robot-aided training;neural network;robotic exoskeletons;post-stroke rehabilitation;sensorimotor impairments;variable assistance;movement execution;movement quality;individualized treatment;kinematic guidance;robotic assistance;voluntary effort;active torques;unmodeled dynamics;passive neuromuscular properties;involuntary forces;muscle activity;high resolution assessment tool;therapy tasks;robustness;Harmony upper-body exoskeleton;motor recovery evaluation;Robot sensing systems;Torque;Shoulder;Muscles;Exoskeletons;Dynamics},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794281&isnumber=8793254

N. Thompson, A. Sinha and G. Krishnan, "Characterizing Architectures of Soft Pneumatic Actuators for a Cable-Driven Shoulder Exoskeleton," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 570-576.
doi: 10.1109/ICRA.2019.8793707
Abstract: Low weight and innate compliance make soft pneumatic actuators an attractive method for actuating wearable robots. Performance of soft pneumatic actuators can be tailored to an application by combining them in novel architectures. We modeled and constructed nested linear and pennate architectures using fiber-reinforced elastomeric enclosures (FREEs) with identical manufacturing parameters and total effective lengths to compare their suitability for a cable-driven exoskeleton for augmenting shoulder flexion. We determined actuator performance requirements using a static model for the transmission of actuator forces to the upper arm via Bowden cables. We experimentally characterized the architectures by measuring their force-displacement curves at a range of pressures, yielding greater force and displacement from the nested architecture in the domain required by our exoskeleton. Results also indicated a force threshold above which the pennate structure produced greater force at any given displacement. We validated the nested linear architecture using a prototype exoskeleton installed on a passive mannequin. Measured joint angles at varying pressures were close to predicted values, adjusted for measured losses due to cable anchor movement.
keywords: {biomechanics;cables (mechanical);elastomers;medical robotics;patient rehabilitation;pneumatic actuators;wearable robots;architectures characterization;wearable robots;fiber-reinforced elastomeric enclosures;FREEs;shoulder flexion;force-displacement curves;cable anchor movement;Bowden cables;nested linear architecture;pennate architectures;cable-driven shoulder exoskeleton;soft pneumatic actuators;Shoulder;Actuators;Force;Exoskeletons;Cable shielding;Torque;Prototypes},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793707&isnumber=8793254

V. Mehrabi, S. F. Atashzar, H. A. Talebi and R. V. Patel, "Design and Implementation of a Two-DOF Robotic System with an Adjustable Force Limiting Mechanism for Ankle Rehabilitation," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 577-583.
doi: 10.1109/ICRA.2019.8794028
Abstract: This paper presents a novel light-weight back-drivable inherently-safe robotic mechanism for delivering ankle rehabilitation therapies. The robot is designed to be used as the ankle module of a multi-purpose lower-limb rehabilitation robot. A novel friction-based safety feature has been introduced that enables mechanical adjustment of the maximum amount of allowable transfer forces and torques to the patient's limb. The design procedure, mathematical modeling and experimental validations are provided to demonstrate the performance of the proposed system.
keywords: {end effectors;friction;manipulator dynamics;medical robotics;mobile robots;patient rehabilitation;patient treatment;lower-limb rehabilitation robot;mechanical adjustment;design procedure;adjustable force limiting mechanism;light-weight back-drivable inherently-safe robotic mechanism;ankle rehabilitation therapies;ankle module;mathematical modeling;experimental validations;two-DOF robotic system;friction-based safety feature;Robots;Wheels;Force;Torque;Medical treatment;Safety;Training},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794028&isnumber=8793254

S. Wang, X. Liu, J. Zhao and H. I. Christensen, "Rorg: Service Robot Software Management with Linux Containers," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 584-590.
doi: 10.1109/ICRA.2019.8793764
Abstract: Scaling up the software system on service robots increases the maintenance burden of developers and the risk of resource contention of the computer embedded on robots. As a result, developers spend much time on configuring, deploying, and monitoring the robot software system; robots may utilize significant computer resources when all software processes are running. We present Rorg, a Linux container-based scheme to manage, schedule, and monitor software components on service robots. Although Linux containers are already widely-used in cloud environments, this technique is challenging to efficiently adopt in service robot systems due to multi-tasking, resource constraints and performance requirements. To pave the way of Linux containers on service robots in an efficient manner, we present a programmable container management interface and a resource time-sharing mechanism incorporated with the Robot Operating System (ROS). Rorg allows developers to pack software into self-contained images and runs them in isolated environments using Linux containers; it also allows the robot to turn on and off software components on demand to avoid resource contention. We evaluate Rorg with a long-term autonomous tour guide robot: It manages 41 software components on the robot and relieved our maintenance burden, and it also reduces CPU load by 45.5% and memory usage by 16.5% on average.
keywords: {cloud computing;control engineering computing;Linux;mobile robots;object-oriented programming;scheduling;service robots;Linux container-based scheme;monitor software components;service robots;Linux containers;service robot systems;resource constraints;programmable container management interface;resource time-sharing mechanism;Rorg;resource contention;long-term autonomous tour guide robot;robot software system;software processes;software components;computer resources;service robot software management;robot operating system;Containers;Software;Linux;Service robots;Data centers;Monitoring},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793764&isnumber=8793254

A. Laurenzi, E. M. Hoffman, L. Muratore and N. G. Tsagarakis, "CartesI/O: A ROS Based Real-Time Capable Cartesian Control Framework," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 591-596.
doi: 10.1109/ICRA.2019.8794464
Abstract: This work introduces a framework for the Cartesian control of multi-legged, highly redundant robots. The proposed framework allows the untrained user to perform complex motion tasks with robotics platforms by leveraging a simple, auto-generated ROS-based interface. Contrary to other motion control frameworks (e.g. ROS MoveIt!), we focus on the execution of Cartesian trajectories that are specified online, rather than planned in advance, as it is the case, for instance, in tele-operation and locomotion tasks. Moreover, we address the problem of generating such motions within a hard real-time (RT) control loop. Finally, we demonstrate the capabilities of our framework both on the COMAN + humanoid robot, and on the hybrid wheeled-legged quadruped CENTAURO.
keywords: {control engineering computing;humanoid robots;legged locomotion;motion control;robot dynamics;robot kinematics;telerobotics;robotics platforms;simple auto-generated ROS-based interface;motion control frameworks;ROS MoveIt;Cartesian trajectories;locomotion tasks;COMAN + humanoid robot;redundant robots;motion tasks;multilegged highly redundant robots;Cartesian control framework;Task analysis;Robot kinematics;Real-time systems;Libraries;C++ languages;Planning},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794464&isnumber=8793254

C. Lesire, S. Roussel, D. Doose and C. Grand, "Synthesis of Real-Time Observers from Past-Time Linear Temporal Logic and Timed Specification," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 597-603.
doi: 10.1109/ICRA.2019.8793754
Abstract: Fault-tolerant architectures are mandatory to ensure the robustness of autonomous robots performing missions in complex and uncertain environments. The first step of a fault-tolerant mechanism is the detection of a faulty behavior of the system. It is then important to provide tools to help robot developers specify relevant observers. It is moreover crucial to guarantee a correct implementation of the observers, i.e. that the observers do not miss data and do not trigger unsuitable recovery actions in case of false detection. In this paper, we propose a specification language for observers that uses Past-Time LTL to express complex formulas on data produced by software components, and timed constraints on the evaluations of these formulas. We moreover provide an implementation of this specification that guarantees a real-time evaluation of the observers. We briefly describe the observers we have specified for a patrolling mission, and we evaluate the performance of our approach compared to state of the art on a benchmark in which we detect errors on a laser range sensor.
keywords: {fault tolerant control;mobile robots;observers;specification languages;temporal logic;timed specification;fault-tolerant architectures;autonomous robots;robot developers;specification language;real-time evaluation;real-time observers;past-time linear temporal logic;past-time LTL;software components;mission patrolling;Observers;Monitoring;Real-time systems;Robot sensing systems;Timing;Safety},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793754&isnumber=8793254

T. Koolen and R. Deits, "Julia for robotics: simulation and real-time control in a high-level programming language," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 604-611.
doi: 10.1109/ICRA.2019.8793875
Abstract: Robotics applications often suffer from the `two-language problem', requiring a low-level language for performance-sensitive components and a high-level language for interactivity and experimentation, which tends to increase software complexity. We demonstrate the use of the Julia programming language to solve this problem by being fast enough for online control of a humanoid robot and flexible enough for prototyping. We present several Julia packages developed by the authors, which together enable roughly 2× realtime simulation of the Boston Dynamics Atlas humanoid robot balancing on flat ground using a quadratic-programming-based controller. Benchmarks show a sufficiently low variation in control frequency to make deployment on the physical robot feasible. We also show that Julia's naturally generic programming style results in versatile packages that are easy to compose and adapt to a wide variety of computational tasks in robotics.
keywords: {computer simulation;control engineering computing;high level languages;humanoid robots;programming languages;quadratic programming;robot dynamics;robot programming;real-time control;high-level programming language;robotics applications;two-language problem;performance-sensitive components;high-level language;software complexity;Julia programming language;online control;Julia packages;Boston Dynamics Atlas humanoid robot;quadratic-programming-based controller;Robots;Libraries;Software packages;C++ languages;Productivity;Resource management},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793875&isnumber=8793254

J. Ichnowski and R. Alterovitz, "Motion Planning Templates: A Motion Planning Framework for Robots with Low-power CPUs," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 612-618.
doi: 10.1109/ICRA.2019.8794099
Abstract: Motion Planning Templates (MPT) is a C++ template-based library that uses compile-time polymorphism to generate robot-specific motion planning code and is geared towards eking out as much performance as possible when running on the low-power CPU of a battery-powered small robot. To use MPT, developers of robot software write or leverage code specific to their robot platform and motion planning problem, and then have MPT generate a robot-specific motion planner and its associated data-structures. The resulting motion planner implementation is faster and uses less memory than general motion planning implementations based upon runtime polymorphism. While MPT loses runtime flexibility, it gains advantages associated with compile-time polymorphism- including the ability to change scalar precision, generate tightly-packed data structures, and store robot-specific data in the motion planning graph. MPT also uses compile-time algorithms to resolve the algorithm implementation, and select the best nearest neighbor algorithm to integrate into it. We demonstrate MPT's performance, lower memory footprint, and ability to adapt to varying robots in motion planning scenarios on a small humanoid robot and on 3D rigid-body motions.
keywords: {control engineering computing;data structures;graph theory;humanoid robots;mobile robots;path planning;low-power CPU;template-based library;compile-time polymorphism;robot-specific motion planning code;robot software;motion planning problem;general motion planning implementations;motion planning graph;compile-time algorithms;motion planning scenarios;humanoid robot;3D rigid-body motions;motion planning framework;MPT;motion planning templates;Planning;Runtime;C++ languages;Libraries;Data structures;Mobile robots},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794099&isnumber=8793254

D. Song and Y. J. Kim, "Distortion-free Robotic Surface-drawing using Conformal Mapping," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 627-633.
doi: 10.1109/ICRA.2019.8794034
Abstract: We present a robotic pen-drawing system that is capable of faithfully reproducing pen art on an unknown surface. Our robotic system relies on an industrial, seven-degree-of freedom manipulator that can be both position- and impedance-controlled. In order to estimate a rough geometry of the target, continuous surface, we first generate a point cloud of the surface using an RGB-D camera, which is filtered to remove outliers and calibrated to the physical canvas surface. Then, our control algorithm physically reproduces digital drawing on the surface by impedance-controlling the manipulator. Our impedance-controlled drawing algorithm compensates for the uncertainty and incompleteness inherent to a point-cloud estimation of the drawing surface. Moreover, since drawing 2D vector pen art on a 3D surface requires surface parameterization that does not destroy the original 2D drawing, we rely on the least squares conformal mapping. Specifically, the conformal map reduces angle distortion during surface parameterization. As a result, our system can create distortion-free and complicated pen drawings on general surfaces with many unpredictable bumps robustly and faithfully.
keywords: {computational geometry;computer graphics;conformal mapping;distance measurement;image reconstruction;least squares approximations;manipulators;mobile robots;solid modelling;distortion-free robotic surface-drawing;robotic pen-drawing system;unknown surface;robotic system;seven-degree-of freedom manipulator;continuous surface;physical canvas surface;point-cloud estimation;drawing surface;2D vector pen art;surface parameterization;squares conformal mapping;complicated pen drawings;general surfaces;impedance-control;digital drawing;2D drawing;Surface impedance;Three-dimensional displays;Robot kinematics;Two dimensional displays;Service robots;Surface reconstruction},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794034&isnumber=8793254

K. Huang, H. K. Chu, B. Lu, J. Lai and L. Cheng, "Automated Cell Patterning System with a Microchip using Dielectrophoresis," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 634-639.
doi: 10.1109/ICRA.2019.8794177
Abstract: The ability to patterning cells is an important technique to facilitate cell-based assay and characterization. In this paper, an automated cell patterning system was developed for the fabrication of large-scale cell patterns. To resolve the challenge of the limited printable area, the cell-printing microchip and the substrate were mounted on the movable stages of the system, and large-scale cell patterns were realized through coordination between the stages. An autofocusing technique was integrated in the system to evaluate the gap between the microchip and the substrate. In order to enhance the performance of the patterning system, different experimental parameters, including the velocity of the moving stage, were examined. Yeast cells suspending in 6-aminohexanoic acid (AHA) solution were considered in this study, and a sequence of characters was successfully printed using the proposed system. The results confirm that this system offers an automatic method with high flexibility to construct large-scale cell patterns for various applications.
keywords: {bioelectric phenomena;biological techniques;cellular biophysics;electrophoresis;lab-on-a-chip;microorganisms;dielectrophoresis;automatic method;6-aminohexanoic acid;yeast cells;cell-printing microchip;large-scale cell patterns;cell-based assay;patterning cells;automated cell patterning system;Integrated circuits;Substrates;Electrodes;Microscopy;Force;Lenses;Glass},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794177&isnumber=8793254

M. E. Helou, S. Mandt, A. Krause and P. Beardsley, "Mobile Robotic Painting of Texture," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 640-647.
doi: 10.1109/ICRA.2019.8793947
Abstract: Robotic painting is well-established in controlled factory environments, but there is now potential for mobile robots to do functional painting tasks around the everyday world. An obvious first target for such robots is painting a uniform single color. A step further is the painting of textured images. Texture involves a varying appearance, and requires that paint is delivered accurately onto the physical surface to produce the desired effect. Robotic painting of texture is relevant for architecture and in themed environments. A key challenge for robotic painting of texture is to take a desired image as input, and to generate the paint commands to as closely as possible create the desired appearance, according to the robotic capabilities. This paper describes a deep learning approach to take an input ink map of a desired texture, and infer robotic paint commands to produce that texture. We analyze the trade-offs between quality of reconstructed appearance and ease of execution. Our method is general for different kinds of robotic paint delivery systems, but the emphasis here is on spray painting. More generally, the framework can be viewed as an approach for solving a specific class of inverse imaging problems.
keywords: {image colour analysis;image reconstruction;image texture;learning (artificial intelligence);mobile robots;neural nets;painting;spraying;mobile robotic painting;mobile robots;robotic paint delivery systems;spray painting;painting tasks;image texture;robotic paint commands;deep learning approach;appearance reconstruction;Painting;Robots;Paints;Ink;Atmospheric modeling;Spraying},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793947&isnumber=8793254

B. Stumph et al., "Detecting Invasive Insects with Unmanned Aerial Vehicles," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 648-654.
doi: 10.1109/ICRA.2019.8794116
Abstract: A key aspect to controlling and reducing the effects invasive insect species have on agriculture is to obtain knowledge about the migration patterns of these species. Current state-of-the-art methods of studying these migration patterns involve a mark-release-recapture technique, in which insects are released after being marked and researchers attempt to recapture them later. However, this approach involves a human researcher manually searching for these insects in large fields and results in very low recapture rates. In this paper, we propose an automated system for detecting released insects using an unmanned aerial vehicle. This system utilizes ultraviolet lighting technology, digital cameras, and lightweight computer vision algorithms to more quickly and accurately detect insects compared to the current state of the art. The efficiency and accuracy that this system provides will allow for a more comprehensive understanding of invasive insect species migration patterns. Our experimental results demonstrate that our system can detect real target insects in field conditions with high precision and recall rates.
keywords: {agriculture;autonomous aerial vehicles;cameras;computer vision;mobile robots;pest control;robot vision;mark-release-recapture technique;invasive insect species migration patterns;unmanned aerial vehicles;computer vision algorithms;invasive insects detection;agriculture;Insects;Cameras;Unmanned aerial vehicles;Image color analysis;Data acquisition;Pipelines;Agriculture},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794116&isnumber=8793254

K. Ok, K. Liu, K. Frey, J. P. How and N. Roy, "Robust Object-based SLAM for High-speed Autonomous Navigation," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 669-675.
doi: 10.1109/ICRA.2019.8794344
Abstract: We present Robust Object-based SLAM for High-speed Autonomous Navigation (ROSHAN), a novel approach to object-level mapping suitable for autonomous navigation. In ROSHAN, we represent objects as ellipsoids and infer their parameters using three sources of information - bounding box detections, image texture, and semantic knowledge - to overcome the observability problem in ellipsoid-based SLAM under common forward-translating vehicle motions. Each bounding box provides four planar constraints on an object surface and we add a fifth planar constraint using the texture on the objects along with a semantic prior on the shape of ellipsoids. We demonstrate ROSHAN in simulation where we outperform the baseline, reducing the median shape error by 83% and the median position error by 72% in a forward-moving camera sequence. We demonstrate similar qualitative result on data collected on a fast-moving autonomous quadrotor.
keywords: {cameras;helicopters;image sequences;image texture;mobile robots;object detection;path planning;robot vision;SLAM (robots);ROSHAN;object-level mapping;ellipsoid-based SLAM;object surface;autonomous quadrotor;bounding box detections;median shape error;forward-moving camera sequence;planar constraint;vehicle motions;semantic knowledge;robust object-based SLAM for high-speed autonomous navigation;Ellipsoids;Image edge detection;Semantics;Simultaneous localization and mapping;Cameras;Shape;Shape measurement},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794344&isnumber=8793254

G. Zogopoulos-Papaliakos, M. Logothetis and K. J. Kyriakopoulos, "A Fault Diagnosis Framework for MAVLink-Enabled UAVs Using Structural Analysis," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 676-682.
doi: 10.1109/ICRA.2019.8793760
Abstract: MAVLink is a popular message protocol for small Unmanned Aerial Vehicles (UAVs). In this work, we present a Fault Detection and Isolation (FDI) framework for fixed-wing UAVs which takes advantage of the information conveyed in MAVLink telemetry streams and produces a bank of residual generators. Structural Analysis is employed to systematically handle the varying set of available measurements, identify the observable faults and adjust the FDI system accordingly. Structural detectability and isolability analyses are carried out. A case-study on a real-life telemetry log of a UAV crash demonstrates the efficacy of the proposed approach.
keywords: {aerospace components;aircraft control;autonomous aerial vehicles;fault diagnosis;telemetry;MAVLink-enabled UAVs;fixed-wing UAVs;MAVLink telemetry streams;residual generators;observable faults;FDI system;isolability analyses;real-life telemetry log;UAV crash;fault diagnosis framework;structural analysis;message protocol;unmanned aerial vehicles;fault detection and isolation framework;Mathematical model;Atmospheric modeling;Generators;Fault diagnosis;Telemetry;Batteries;Unmanned aerial vehicles},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793760&isnumber=8793254

M. M. d. Almeida, R. Moghe and M. Akella, "Real-Time Minimum Snap Trajectory Generation for Quadcopters: Algorithm Speed-up Through Machine Learning," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 683-689.
doi: 10.1109/ICRA.2019.8793569
Abstract: This paper addresses the problem of generating quadcopter minimum snap trajectories for real time applications. Previous efforts addressed this problem by either employing a gradient descent method, or by greatly sacrificing optimality for faster solutions that are amenable for onboard implementation. In this work, outputs of the gradient descent method are used offline to train a supervised neural network. We show that the use of neural networks results typically in two orders of magnitude reduction in computational time. Our proposed approach can be used for warm-starting onboard implementable iterative methods with an “educated ” initial guess. This work is motivated by the application for human-machine interface in which a human provides desired trajectory through a smart-tablet interface, which has to be translated into a dynamically feasible trajectory for a quadcopter. The proposed solution is tested in thousands of different examples, demonstrating its effectiveness as a booster for minimum snap trajectory generation for quadcopters.
keywords: {computational complexity;control engineering computing;gradient methods;helicopters;iterative methods;learning (artificial intelligence);neurocontrollers;trajectory control;human-machine interface;gradient descent method;supervised neural network;computational time;machine learning;quadcopter;iterative methods;real-time minimum snap trajectory generation;smart-tablet interface;Trajectory;Resource management;Real-time systems;Neural networks;Acceleration;Quadratic programming;Nonlinear optics},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793569&isnumber=8793254

E. Kaufmann et al., "Beauty and the Beast: Optimal Methods Meet Learning for Drone Racing," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 690-696.
doi: 10.1109/ICRA.2019.8793631
Abstract: Autonomous micro aerial vehicles still struggle with fast and agile maneuvers, dynamic environments, imperfect sensing, and state estimation drift. Autonomous drone racing brings these challenges to the fore. Human pilots can fly a previously unseen track after a handful of practice runs. In contrast, state-of-the-art autonomous navigation algorithms require either a precise metric map of the environment or a large amount of training data collected in the track of interest. To bridge this gap, we propose an approach that can fly a new track in a previously unseen environment without a precise map or expensive data collection. Our approach represents the global track layout with coarse gate locations, which can be easily estimated from a single demonstration flight. At test time, a convolutional network predicts the poses of the closest gates along with their uncertainty. These predictions are incorporated by an extended Kalman filter to maintain optimal maximum-a-posteriori estimates of gate locations. This allows the framework to cope with misleading high-variance estimates that could stem from poor observability or lack of visible gates. Given the estimated gate poses, we use model predictive control to quickly and accurately navigate through the track. We conduct extensive experiments in the physical world, demonstrating agile and robust flight through complex and diverse previously-unseen race tracks. The presented approach was used to win the IROS 2018 Autonomous Drone Race Competition, outracing the second-placing team by a factor of two.
keywords: {autonomous aerial vehicles;collision avoidance;Kalman filters;learning (artificial intelligence);mobile robots;navigation;optimisation;predictive control;state estimation;robust flight;previously-unseen race tracks;optimal methods;fast maneuvers;agile maneuvers;dynamic environments;imperfect sensing;state estimation drift;human pilots;unseen track;practice runs;state-of-the-art autonomous navigation algorithms;precise metric map;training data;unseen environment;precise map;expensive data collection;global track layout;coarse gate locations;single demonstration flight;convolutional network;closest gates;extended Kalman filter;maximum-a-posteriori estimates;high-variance estimates;poor observability;visible gates;estimated gate poses;model predictive control;agile flight;autonomous microaerial vehicles;autonomous drone racing;IROS 2018 autonomous drone race competition;Logic gates;Drones;Navigation;Training data;Current measurement;Layout;Uncertainty},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793631&isnumber=8793254

A. Stambler, G. Sherwin and P. Rowe, "Detection and Reconstruction of Wires Using Cameras for Aircraft Safety Systems," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 697-703.
doi: 10.1109/ICRA.2019.8793526
Abstract: We extend the ability of cameras to perceive obstacles for aircraft safety systems by enabling 3d sensing of free hanging wires. Our algorithm exploits the specialized 2d and 3d structure of wires to exceed state of the art performance in 2d sensing and 3d location estimation of wire obstacles. In 2d, a new neural network architecture, Deep Wire CNN, directly predicts the location of wire line segments in the image. In 3d, the detections are tracked and triangulated as the aircraft flies in order to estimate the wire's location. Our triangulation uses a new formulation of wire reconstruction as the estimation of the wire's vertical plane. Together these advancements enable real-time detections of wire hazards at ranges of over 1km. The system performance is evaluated on prior image level wire detection datasets and we introduce a new public dataset in order to evaluate full system results on over 40 approaches to power lines from a manned helicopter.
keywords: {aerospace computing;aerospace safety;aircraft;convolutional neural nets;feature extraction;hazards;image reconstruction;image segmentation;neural net architecture;object detection;real-time systems;cameras;aircraft safety systems;free hanging wires;wire obstacles;neural network architecture;Deep Wire CNN;wire line segments;wire reconstruction;real-time detections;wire hazards;wire detection;Wires;Image reconstruction;Cameras;Detectors;Image resolution;Agriculture;Three-dimensional displays},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793526&isnumber=8793254

S. Park, Y. Lee, J. Heo and D. Lee, "Pose and Posture Estimation of Aerial Skeleton Systems for Outdoor Flying," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 704-710.
doi: 10.1109/ICRA.2019.8794080
Abstract: We present a novel pose and posture estimation framework of aerial skeleton system for outdoor flying. To exploit redundant/independent sensing while rendering the system “modular”, we attach an IMU (inertial measurement unit) sensor and a GNSS (global navigation satellite system) module on each link and perform SE(3)-motion EKF (extended Kalman filtering). We then apply the kinematic constraints of the aerial skeleton system to these EKF estimates of all the links through SCKF (smoothly constrained Kalman filtering), thereby, enforcing the kinematic coherency of the skeleton system and, consequently, significantly enhancing the estimation accuracy and the control performance/stability of the aerial skeleton system. A semi-distributed version of the obtained estimation framework is also presented to address the issue of scalability. The theory is then verified/demonstrated with real outdoor flying experiments and simulation studies of a three-link aerial skeleton system.
keywords: {Kalman filters;pose estimation;satellite navigation;outdoor flying;posture estimation framework;system modular;GNSS module;global navigation satellite system;EKF estimates;three-link aerial skeleton system;inertial measurement unit;extended Kalman filtering;Skeleton;Estimation;Kinematics;Global navigation satellite system;Rotors;Kalman filters;Force},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794080&isnumber=8793254

M. Coombes, W. Chen and C. Liu, "Flight Testing Boustrophedon Coverage Path Planning for Fixed Wing UAVs in Wind," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 711-717.
doi: 10.1109/ICRA.2019.8793943
Abstract: A method was previously developed by this author to optimise the flight path of a fixed wing UAV performing aerial surveys of complex concave agricultural fields. This relies heavily on a flight time in wind prediction model as its cost function. This paper aims to validate this model by comparing flight test results with the model prediction. There are a number of assumptions that this model relies on. The major assumption is that wind is steady and uniform over the small area and time scales involved in a survey. To show that this is reasonable, wind fields measurements will be taken from a multi rotor UAV with an ultrasonic windspeed sensor.
keywords: {aerospace components;autonomous aerial vehicles;mobile robots;path planning;remotely operated vehicles;wind;flight test results;model prediction;multirotor UAV;boustrophedon coverage path planning;flight path;aerial surveys;complex concave agricultural fields;flight time;wind prediction model;cost function;wind field measurements;fixed wing UAV;Wind;Aircraft;Robot sensing systems;Mathematical model;Atmospheric modeling;Path planning;Predictive models;Aerial Surveying;Coverage Path Planning;Remote Sensing;Boustrophedon paths;Wind;Trochoids},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793943&isnumber=8793254

A. A. Meera, M. Popović, A. Millane and R. Siegwart, "Obstacle-aware Adaptive Informative Path Planning for UAV-based Target Search," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 718-724.
doi: 10.1109/ICRA.2019.8794345
Abstract: Target search with unmanned aerial vehicles (UAVs) is relevant problem to many scenarios, e.g., search and rescue (SaR). However, a key challenge is planning paths for maximal search efficiency given flight time constraints. To address this, we propose the Obstacle-aware Adaptive Informative Path Planning (OA-IPP) algorithm for target search in cluttered environments using UAVs. Our approach leverages a layered planning strategy using a Gaussian Process (GP)based model of target occupancy to generate informative paths in continuous 3D space. Within this framework, we introduce an adaptive replanning scheme which allows us to trade off between information gain, field coverage, sensor performance, and collision avoidance for efficient target detection. Extensive simulations show that our OA-IPP method performs better than state-of-the-art planners, and we demonstrate its application in a realistic urban SaR scenario.
keywords: {aircraft control;autonomous aerial vehicles;collision avoidance;Gaussian processes;mobile robots;object detection;robot vision;target occupancy;UAV-based target search;Gaussian process based model;flight time constraints;planning strategy;obstacle-aware adaptive informative path planning algorithm;target detection;unmanned aerial vehicles;collision avoidance;Planning;Search problems;Three-dimensional displays;Optimization;Trajectory;Unmanned aerial vehicles;Robot sensing systems},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794345&isnumber=8793254

J. Tordesillas, B. T. Lopez, J. Carter, J. Ware and J. P. How, "Real-Time Planning with Multi-Fidelity Models for Agile Flights in Unknown Environments," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 725-731.
doi: 10.1109/ICRA.2019.8794248
Abstract: Autonomous navigation through unknown environments is a challenging task that entails real-time localization, perception, planning, and control. UAVs with this capability have begun to emerge in the literature with advances in lightweight sensing and computing. Although the planning methodologies vary from platform to platform, many algorithms adopt a hierarchical planning architecture where a slow, low-fidelity global planner guides a fast, high-fidelity local planner. However, in unknown environments, this approach can lead to erratic or unstable behavior due to the interaction between the global planner, whose solution is changing constantly, and the local planner; a consequence of not capturing higher-order dynamics in the global plan. This work proposes a planning framework in which multi-fidelity models are used to reduce the discrepancy between the local and global planner. Our approach uses high-, medium-, and low-fidelity models to compose a path that captures higher-order dynamics while remaining computationally tractable. In addition, we address the interaction between a fast planner and a slower mapper by considering the sensor data not yet fused into the map during the collision check. This novel mapping and planning framework for agile flights is validated in simulation and hardware experiments, showing replanning times of 5-40 ms in cluttered environments.
keywords: {autonomous aerial vehicles;collision avoidance;mobile robots;sensors;low-fidelity models;fast planner;planning framework;agile flights;replanning times;cluttered environments;multifidelity models;autonomous navigation;real-time localization;lightweight sensing;planning methodologies;hierarchical planning architecture;low-fidelity global planner;high-fidelity local planner;erratic behavior;unstable behavior;global plan;higher-order dynamics;real-time planning;sensor data;collision check;UAV;time 5.0 ms to 40.0 ms;Planning;Trajectory;Computational modeling;Vehicle dynamics;Robot sensing systems;Optimization},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794248&isnumber=8793254

M. Ryll, J. Ware, J. Carter and N. Roy, "Efficient Trajectory Planning for High Speed Flight in Unknown Environments," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 732-738.
doi: 10.1109/ICRA.2019.8793930
Abstract: There has been considerable recent work in motion planning for UAVs to enable aggressive, highly dynamic flight in known environments with motion capture systems. However, these existing planners have not been shown to enable the same kind of flight in unknown, outdoor environments. In this paper we present a receding horizon planning architecture that enables the fast replanning necessary for reactive obstacle avoidance by combining three techniques. First, we show how previous work in computationally efficient, closed-form trajectory generation method can be coupled with spatial partitioning data structures to reason about the geometry of the environment in real-time. Second, we show how to maintain safety margins during fast flight in unknown environments by planning velocities according to obstacle density. Third, our receding-horizon, sampling-based motion planner uses minimum-jerk trajectories and closed-loop tracking to enable smooth, robust, high-speed flight with the low angular rates necessary for accurate visual-inertial navigation. We compare against two state-of-the-art, reactive motion planners in simulation and benchmark solution quality against an offline global planner. Finally, we demonstrate our planner over 80 flights with a combined distance of 22km of autonomous quadrotor flights in an urban environment at speeds up to 9.4ms $^{-1}$.
keywords: {aircraft control;autonomous aerial vehicles;closed loop systems;collision avoidance;inertial navigation;mobile robots;motion control;optimal control;predictive control;sampling methods;trajectory control;motion planning;motion capture systems;receding horizon planning architecture;reactive obstacle avoidance;closed-form trajectory generation method;spatial partitioning data structures;obstacle density;sampling-based motion planner;minimum-jerk trajectories;closed-loop tracking;high-speed flight;autonomous quadrotor flights;urban environment;trajectory planning;visual-inertial navigation;distance 22.0 km;Trajectory;Planning;Sensors;Three-dimensional displays;Cameras;Vehicle dynamics;Tracking},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793930&isnumber=8793254

V. L. J. Somers and I. R. Manchester, "Priority Maps for Surveillance and Intervention of Wildfires and other Spreading Processes," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 739-745.
doi: 10.1109/ICRA.2019.8793874
Abstract: Unmanned Aerial Vehicle (UAV) path planning algorithms often assume a knowledge reward function or priority map, indicating the most important areas to visit. In this paper we propose a method to create priority maps for monitoring or intervention of dynamic spreading processes such as wildfires. The presented optimization framework utilizes the properties of positive systems, in particular the separable structure of value (cost-to-go) functions, to provide scalable algorithms for surveillance and intervention. We present results obtained for a 16 and 1000 node example and convey how the priority map responds to changes in the dynamics of the system. The larger example of 1000 nodes, representing a fictional landscape, shows how the method can integrate bushfire spreading dynamics, landscape and wind conditions. Finally, we give an example of combining the proposed method with a travelling salesman problem for UAV path planning for wildfire intervention.
keywords: {autonomous aerial vehicles;optimisation;path planning;wildfires;priority maps;wildfires;knowledge reward function;dynamic spreading processes;surveillance;bushfire spreading dynamics;wildfire intervention;unmanned aerial vehicle;optimization framework;UAV path planning;Mathematical model;Surveillance;Stochastic processes;Unmanned aerial vehicles;Path planning;Vehicle dynamics;Computational modeling},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793874&isnumber=8793254

M. Vecerik, O. Sushkov, D. Barker, T. Rothörl, T. Hester and J. Scholz, "A Practical Approach to Insertion with Variable Socket Position Using Deep Reinforcement Learning," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 754-760.
doi: 10.1109/ICRA.2019.8794074
Abstract: Insertion is a challenging haptic and visual control problem with significant practical value for manufacturing. Existing approaches in the model-based robotics community can be highly effective when task geometry is known, but are complex and cumbersome to implement, and must be tailored to each individual problem by a qualified engineer. Within the learning community there is a long history of insertion research, but existing approaches are either too sample-inefficient to run on real robots, or assume access to high-level object features, e.g. socket pose. In this paper we show that relatively minor modifications to an off-the-shelf Deep-RL algorithm (DDPG), combined with a small number of human demonstrations, allows the robot to quickly learn to solve these tasks efficiently and robustly. Our approach requires no modeling or simulation, no parameterized search or alignment behaviors, no vision system aside from raw images, and no reward shaping. We evaluate our approach on a narrow-clearance peg-insertion task and a deformable clip-insertion task, both of which include variability in the socket position. Our results show that these tasks can be solved reliably on the real robot in less than 10 minutes of interaction time, and that the resulting policies are robust to variance in the socket position and orientation.
keywords: {control engineering computing;industrial robots;learning (artificial intelligence);neural nets;production engineering computing;variable socket position;visual control problem;model-based robotics community;task geometry;off-the-shelf Deep-RL algorithm;narrow-clearance peg-insertion task;deformable clip-insertion task;deep reinforcement learning;haptic control problem;Task analysis;Robots;Sockets;Visualization;Training;Plugs;Feature extraction},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794074&isnumber=8793254

Y. Cui, D. Isele, S. Niekum and K. Fujimura, "Uncertainty-Aware Data Aggregation for Deep Imitation Learning," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 761-767.
doi: 10.1109/ICRA.2019.8794025
Abstract: Estimating statistical uncertainties allows autonomous agents to communicate their confidence during task execution and is important for applications in safety-critical domains such as autonomous driving. In this work, we present the uncertainty-aware imitation learning (UAIL) algorithm for improving end-to-end control systems via data aggregation. UAIL applies Monte Carlo Dropout to estimate uncertainty in the control output of end-to-end systems, using states where it is uncertain to selectively acquire new training data. In contrast to prior data aggregation algorithms that force human experts to visit sub-optimal states at random, UAIL can anticipate its own mistakes and switch control to the expert in order to prevent visiting a series of sub-optimal states. Our experimental results from simulated driving tasks demonstrate that our proposed uncertainty estimation method can be leveraged to reliably predict infractions. Our analysis shows that UAIL outperforms existing data aggregation algorithms on a series of benchmark tasks.
keywords: {data aggregation;learning (artificial intelligence);Monte Carlo methods;uncertain systems;uncertainty estimation method;UAIL;uncertainty-aware data aggregation;deep imitation learning;statistical uncertainties;autonomous agents;task execution;safety-critical domains;autonomous driving;uncertainty-aware imitation learning algorithm;end-to-end control systems;Monte Carlo Dropout;control output;end-to-end systems;training data;prior data aggregation algorithms;sub-optimal states;simulated driving tasks;Uncertainty;Data aggregation;Task analysis;Switches;Estimation;Data models},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794025&isnumber=8793254

S. Thakur, H. van Hoof, J. C. G. Higuera, D. Precup and D. Meger, "Uncertainty Aware Learning from Demonstrations in Multiple Contexts using Bayesian Neural Networks," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 768-774.
doi: 10.1109/ICRA.2019.8794328
Abstract: Diversity of environments is a key challenge that causes learned robotic controllers to fail due to the discrepancies between the training and evaluation conditions. Training from demonstrations in various conditions can mitigate - but not completely prevent - such failures. Learned controllers such as neural networks typically do not have a notion of uncertainty that allows to diagnose an offset between training and testing conditions, and potentially intervene. In this work, we propose to use Bayesian Neural Networks, which have such a notion of uncertainty. We show that uncertainty can be leveraged to consistently detect situations in high-dimensional simulated and real robotic domains in which the performance of the learned controller would be sub-par. Also, we show that such an uncertainty based solution allows making an informed decision about when to invoke a fallback strategy. One fallback strategy is to request more data. We empirically show that providing data only when requested results in increased data-efficiency.
keywords: {Bayes methods;belief networks;intelligent robots;learning (artificial intelligence);learning systems;neurocontrollers;Bayesian neural networks;robotic controllers;evaluation conditions;learned controller;testing conditions;high-dimensional simulated domains;real robotic domains;uncertainty based solution;uncertainty aware learning;Uncertainty;Task analysis;Neural networks;Training;Robots;Bayes methods;Measurement uncertainty},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794328&isnumber=8793254

F. Behbahani et al., "Learning From Demonstration in the Wild," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 775-781.
doi: 10.1109/ICRA.2019.8794412
Abstract: Learning from demonstration (LfD) is useful in settings where hand-coding behaviour or a reward function is impractical. It has succeeded in a wide range of problems but typically relies on manually generated demonstrations or specially deployed sensors and has not generally been able to leverage the copious demonstrations available in the wild: those that capture behaviours that were occurring anyway using sensors that were already deployed for another purpose, e.g., traffic camera footage capturing demonstrations of natural behaviour of vehicles, cyclists, and pedestrians. We propose video to behaviour (ViBe), a new approach to learn models of behaviour from unlabelled raw video data of a traffic scene collected from a single, monocular, initially uncalibrated camera with ordinary resolution. Our approach calibrates the camera, detects relevant objects, tracks them through time, and uses the resulting trajectories to perform LfD, yielding models of naturalistic behaviour. We apply ViBe to raw videos of a traffic intersection and show that it can learn purely from videos, without additional expert knowledge.
keywords: {cameras;learning (artificial intelligence);object detection;traffic engineering computing;video signal processing;uncalibrated camera;learning from demonstration;ViBe;traffic intersection;knowledge expert;video to behaviour;natural behaviour;reward function;hand-coding behaviour;wild;raw videos;naturalistic behaviour;LfD;monocular camera;single camera;traffic scene;Cameras;Trajectory;Roads;Three-dimensional displays;Sensors;Training;Tracking},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794412&isnumber=8793254

H. Bharadhwaj, Z. Wang, Y. Bengio and L. Paull, "A Data-Efficient Framework for Training and Sim-to-Real Transfer of Navigation Policies," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 782-788.
doi: 10.1109/ICRA.2019.8794310
Abstract: Learning effective visuomotor policies for robots purely from data is challenging, but also appealing since a learning-based system should not require manual tuning or calibration. In the case of a robot operating in a real environment the training process can be costly, time-consuming, and even dangerous since failures are common at the start of training. For this reason, it is desirable to be able to leverage simulation and off-policy data to the extent possible to train the robot. In this work, we introduce a robust framework that plans in simulation and transfers well to the real environment. Our model incorporates a gradient-descent based planning module, which, given the initial image and goal image, encodes the images to a lower dimensional latent state and plans a trajectory to reach the goal. The model, consisting of the encoder and planner modules, is first trained through a meta-learning strategy in simulation. We subsequently perform adversarial domain transfer on the encoder by using a bank of unlabelled but random images from the simulation and real environments to enable the encoder to map images from the real and simulated environments to a similarly distributed latent representation. By fine tuning the entire model (encoder + planner) with only a few real world expert demonstrations, we show successful planning performances in different navigation tasks.
keywords: {gradient methods;image coding;learning (artificial intelligence);mobile robots;path planning;data-efficient framework;sim-to-real transfer;navigation policies;effective visuomotor policies;learning-based system;manual tuning;robot operating;training process;leverage simulation;off-policy data;initial image;lower dimensional latent state;planner modules;meta-learning strategy;adversarial domain transfer;simulated environments;similarly distributed latent representation;fine tuning;encoder + planner;planning performances;navigation tasks;unlabelled random images;Robots;Data models;Planning;Task analysis;Trajectory;Training;Navigation},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794310&isnumber=8793254

R. P. Bhattacharyya, D. J. Phillips, C. Liu, J. K. Gupta, K. Driggs-Campbell and M. J. Kochenderfer, "Simulating Emergent Properties of Human Driving Behavior Using Multi-Agent Reward Augmented Imitation Learning," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 789-795.
doi: 10.1109/ICRA.2019.8793750
Abstract: Recent developments in multi-agent imitation learning have shown promising results for modeling the behavior of human drivers. However, it is challenging to capture emergent traffic behaviors that are observed in real-world datasets. Such behaviors arise due to the many local interactions between agents that are not commonly accounted for in imitation learning. This paper proposes Reward Augmented Imitation Learning (RAIL), which integrates reward augmentation into the multi-agent imitation learning framework and allows the designer to specify prior knowledge in a principled fashion. We prove that convergence guarantees for the imitation learning process are preserved under the application of reward augmentation. This method is validated in a driving scenario, where an entire traffic scene is controlled by driving policies learned using our proposed algorithm. Further, we demonstrate improved performance in comparison to traditional imitation learning algorithms both in terms of the local actions of a single agent and the behavior of emergent properties in complex, multi-agent settings.
keywords: {behavioural sciences computing;convergence;learning (artificial intelligence);multi-agent systems;traffic engineering computing;multiagent settings;multiagent imitation learning;human drivers;reward augmentation;imitation learning process;multiagent reward augmented imitation learning;traffic behaviors;imitation learning algorithms;human driving behavior modeling;prior knowledge specification;convergence guarantees;driving policies;Rails;Convergence;Biological system modeling;Trajectory;Computational modeling;Autonomous vehicles},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793750&isnumber=8793254

C. Sweeney, G. Izatt and R. Tedrake, "A Supervised Approach to Predicting Noise in Depth Images," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 796-802.
doi: 10.1109/ICRA.2019.8793820
Abstract: Modern robotic systems are very complex and need to be tested in simulations with detailed sensor noise models to effectively verify robotic behavior. Depth imagery in particular comes with significant noise in the form of scene-dependent pixel-wise dropouts and distortions. Unfortunately, many depth camera simulations contain limited noise models, or can only support generating realistic depth images of simple scenes, which limits their usefulness in effectively testing perception algorithms. We propose a data driven approach to generate more realistic noise for complex simulated environments by using a convolutional neural network (CNN) to predict which pixels of a simulated noise-free depth image will not have returns (no-depth-return pixels, or NDP). We choose to focus on NDP here, as these dropouts are the most common and dramatic form of depth image noise. To train this network, we use reconstructed real-world scenes from the Label Fusion dataset to provide ground truth depth for each noisy depth image used to scan the scene. We use the resulting noise-free and noisy depth image pairs as labeled examples and train the network to predict which pixels of the noise-free image will be NDP. When used to post-process a simulation of a depth sensor, this system produces realistic depth images, even in cluttered scenes. To demonstrate that our approach successfully closes the reality gap for depth imagery, we show that the popular ICP algorithm for object pose estimation fails more realistically on our CNN-corrupted simulated depth images than on uncorrupted depth images and unsupervised domain adaptation baselines.
keywords: {cameras;convolutional neural nets;image denoising;image fusion;image reconstruction;image sensors;object detection;pose estimation;robot vision;supervised learning;supervised approach;modern robotic systems;detailed sensor noise models;robotic behavior;scene-dependent pixel-wise dropouts;depth camera simulations;data driven approach;convolutional neural network;no-depth-return pixels;NDP;ground truth depth;noisy depth image;resulting noise-free;noise-free image;depth sensor;cluttered scenes;uncorrupted depth images;noise prediction;scenes reconstruction;CNN;unsupervised domain adaptation baselines;object pose estimation;noise-free depth image;label fusion dataset;Cameras;Image reconstruction;Data models;Noise measurement;Robot vision systems},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793820&isnumber=8793254

C. Petschnigg, M. Brandstötter, H. Pichler, M. Hofbaur and B. Dieber, "Quantum Computation in Robotic Science and Applications," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 803-810.
doi: 10.1109/ICRA.2019.8793768
Abstract: Using the effects of quantum mechanics for computing challenges has been an often discussed topic for decades. The frequent successes and early products in this area, which we have seen in recent years, indicate that we are currently entering a new era of computing. This paradigm shift will also impact the work of robotic scientists and the applications of robotics. New possibilities as well as new approaches to known problems will enable the creation of even more powerful and intelligent robots that make use of quantum computing cloud services or co-processors. In this position paper, we discuss potential application areas and also point out open research topics in quantum computing for robotics. We go into detail on the impact of quantum computing in artificial intelligence and machine learning, sensing and perception, kinematics as well as system diagnosis. For each topic we point out where quantum computing could be applied based on results from current research.
keywords: {cloud computing;intelligent robots;learning (artificial intelligence);quantum computing;robot programming;quantum computing;cloud services;artificial intelligence;machine learning;robotic scientists;quantum mechanics;quantum computation;intelligent robots;powerful robots;Robot sensing systems;Computers;Optimization;Qubit;Acceleration},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793768&isnumber=8793254

Y. Fan, J. Luo and M. Tomizuka, "A Learning Framework for High Precision Industrial Assembly," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 811-817.
doi: 10.1109/ICRA.2019.8793659
Abstract: Automatic assembly has broad applications in industries. Traditional assembly tasks utilize predefined trajectories or tuned force control parameters, which make the automatic assembly time-consuming, difficult to generalize, and not robust to uncertainties. In this paper, we propose a learning framework for high precision industrial assembly. The framework combines both the supervised learning and the reinforcement learning. The supervised learning utilizes trajectory optimization to provide the initial guidance to the policy, while the reinforcement learning utilizes actor-critic algorithm to establish the evaluation system even the supervisor is not accurate. The proposed learning framework is more efficient compared with the reinforcement learning and achieves better stability performance than the supervised learning. The effectiveness of the method is verified by both the simulation and experiment. Experimental videos are available at [1].
keywords: {assembling;optimisation;production engineering computing;supervised learning;learning framework;high precision industrial assembly;reinforcement learning;automatic assembly;supervised learning;assembly tasks;trajectory optimization;actor-critic algorithm;Task analysis;Trajectory;Optimization;Dynamics;Supervised learning;Computational modeling;Space exploration},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793659&isnumber=8793254

S. Tian et al., "Manipulation by Feel: Touch-Based Control with Deep Predictive Models," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 818-824.
doi: 10.1109/ICRA.2019.8794219
Abstract: Touch sensing is widely acknowledged to be important for dexterous robotic manipulation, but exploiting tactile sensing for continuous, non-prehensile manipulation is challenging. General purpose control techniques that are able to effectively leverage tactile sensing as well as accurate physics models of contacts and forces remain largely elusive, and it is unclear how to even specify a desired behavior in terms of tactile percepts. In this paper, we take a step towards addressing these issues by combining high-resolution tactile sensing with data-driven modeling using deep neural network dynamics models. We propose deep tactile MPC, a framework for learning to perform tactile servoing from raw tactile sensor inputs, without manual supervision. We show that this method enables a robot equipped with a GelSight-style tactile sensor to manipulate a ball, analog stick, and 20-sided die, learning from unsupervised autonomous interaction and then using the learned tactile predictive model to reposition each object to user-specified configurations, indicated by a goal tactile reading. Videos, visualizations and the code are available here: https://sites.google.com/view/deeptactilempc.
keywords: {dexterous manipulators;neurocontrollers;predictive control;tactile sensors;unsupervised learning;feel;touch-based control;deep predictive models;touch sensing;dexterous robotic manipulation;tactile sensing;nonprehensile manipulation;general purpose control techniques;accurate physics models;tactile percepts;high-resolution tactile;deep neural network dynamics models;deep tactile MPC;tactile servoing;raw tactile sensor inputs;GelSight-style tactile sensor;learned tactile predictive model;user-specified configurations;goal tactile reading;Task analysis;Predictive models;Tactile sensors;Videos},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794219&isnumber=8793254

B. W. K. Ang and C. Yeow, "3D Printed Soft Pneumatic Actuators with Intent Sensing for Hand Rehabilitative Exoskeletons*," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 841-846.
doi: 10.1109/ICRA.2019.8793785
Abstract: Loss of functional motor skills are common and often require patients to undergo rehabilitation so that they have a chance at motor recovery. Advancement in technology has seen to a rise in the use of robotic technology in conducting rehabilitative exercises that are traditionally carried out by physiotherapists. In recent years, soft robotic exoskeletons, using pneumatic-based actuation in particular, have gained much interest due to their compliant characteristics and safe operating conditions. In order to carry out complex task-based rehabilitative exercises, these soft pneumatic actuators must ideally be able to move with multiple degrees of freedom or minimally, in a bidirectional motion. Majority of the research covering soft actuators can only achieve finger flexion with some providing passive finger extension. Non-invasive intent detection in the control of these exoskeletons is also lacking in sensing both finger flexion and extension. In this paper we present our work on a fold-based bidirectional 3D printed intent-sensing soft pneumatic actuator (ISPA) that can achieve bidirectional motion and provide intent detection for finger flexion and extension for application in upper limb rehabilitative exoskeletons.
keywords: {biomechanics;medical robotics;patient rehabilitation;pneumatic actuators;intent sensing;hand rehabilitative exoskeletons;functional motor skills;motor recovery;soft robotic exoskeletons;complex task-based rehabilitative exercises;bidirectional motion;soft actuators;finger flexion;noninvasive intent detection;upper limb rehabilitative exoskeletons;passive finger extension;fold-based bidirectional 3D printed intent-sensing soft pneumatic actuator;Fingers;Exoskeletons;Actuators;Robots;Three-dimensional displays;Optical sensors},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793785&isnumber=8793254

A. Shafti, P. Orlov and A. A. Faisal, "Gaze-based, Context-aware Robotic System for Assisted Reaching and Grasping," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 863-869.
doi: 10.1109/ICRA.2019.8793804
Abstract: Assistive robotic systems endeavour to support those with movement disabilities, enabling them to move again and regain functionality. Main issue with these systems is the complexity of their low-level control, and how to translate this to simpler, higher level commands that are easy and intuitive for a human user to interact with. We have created a multi-modal system, consisting of different sensing, decision making and actuating modalities, leading to intuitive, human-in-the-loop assistive robotics. The system takes its cue from the user's gaze, to decode their intentions and implement low-level motion actions to achieve high-level tasks. This results in the user simply having to look at the objects of interest, for the robotic system to assist them in reaching for those objects, grasping them, and using them to interact with other objects. We present our method for 3D gaze estimation, and grammars-based implementation of sequences of action with the robotic system. The 3D gaze estimation is evaluated with 8 subjects, showing an overall accuracy of 4.68\pm 0.14cm. The full system is tested with 5 subjects, showing successful implementation of 100% of reach to gaze point actions and full implementation of pick and place tasks in 96%, and pick and pour tasks in 76% of cases. Finally we present a discussion on our results and what future work is needed to improve the system.
keywords: {gaze tracking;grammars;handicapped aids;human-robot interaction;medical robotics;mobile robots;patient rehabilitation;human-in-the-loop assistive robotics;low-level motion actions;3D gaze estimation;grammars-based implementation;gaze-based;context-aware robotic system;assistive robotic systems;movement disabilities;human user;multimodal system;assisted reaching;assisted grasping;Three-dimensional displays;Cameras;Robot kinematics;Robot vision systems;Grasping},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793804&isnumber=8793254

J. Fong, C. Martinez and M. Tavakoli, "Ways to Learn a Therapist’s Patient-specific Intervention: Robotics-vs Telerobotics-mediated Hands-on Teaching," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 870-876.
doi: 10.1109/ICRA.2019.8793907
Abstract: Due to the limitations of therapists time and healthcare resources to cover the increasing demand for rehabilitation services, robot-assisted rehabilitation is becoming an appealing, powerful and economical solution. In our previous research, a solution that combines Learning from Demonstration (LfD) and robotic rehabilitation to save the therapists time and reduce the therapy costs was proposed. In this paper we compare two modalities, Robot-and Telerobotic-Mediated Kinesthetic Teaching (RMKT and TMKT), for implementing LfD in robotic rehabilitation. Our results show that behaviors demonstrated in both modalities are able to be imitated accurately, but demonstrations in TMKT have less repeatability.
keywords: {medical robotics;patient rehabilitation;patient treatment;telerobotics;therapists time;healthcare resources;rehabilitation services;robot-assisted rehabilitation;economical solution;LfD;robotic rehabilitation;learning from demonstration;telerobotic-mediated hands-on teaching;telerobotic-mediated kinesthetic teaching;TMKT;RMKT;Task analysis;Impedance;Medical treatment;Robot kinematics;Robot sensing systems;Service robots},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8793907&isnumber=8793254

D. Anopas et al., "Development of a Novel Force Sensing System to Measure the Ground Reaction Force of Rats with Complete Spinal Cord Injury," 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 877-882.
doi: 10.1109/ICRA.2019.8794368
Abstract: To date, the aim of spinal cord injury (SCI) researches in animals is to find the most effective treatment method which can lead to faster recovery. In order to evaluate if the method is effective, robust functional assessments are crucial. From the past to present, indicators to observe the recovery of the motor function in rodent SCI models are using human observance or the Basso, Beattie, and Bresnahan score (BBB score), force detection, and imaging approaches. Nevertheless, these indicators do not meet some requirements for a severe full transection injury case. The goal of this project is to develop a novel force sensing system for measuring the ground reaction force of rats with severe SCI. In total, this system was tested with 12 spinalized rats. Following a full transection at the T9-T10 level of the spinal cord in rats with a 2mm gap, a nanofiber scaffold containing Neurotrophin-3 (NT-3), as previously described, was implanted [1]. After 12 weeks of rehabilitative training, results showed that rats that underwent rehabilitation were able to gradually exert more force as compared to rats that did not undergo rehabilitation. At Week 6, the ground reaction force recorded in rats with rehabilitation was 0.8 ± 0.1 N in left limb and 0.75 ± 0.14 N in right limb. On the other hand, rats without rehabilitation exerted 0.52 ± 0.06 N in left limb and 0.47 ± 0.09 N in right limb. At Week 12, the force recorded in rehabilitated rats increased to 1.43 ± 0.13 N in left limb and 1.28 ± 0.17 N in right limb whereas in rats without rehabilitation, the force recorded was only 0.74 ± 0.12 N in left limb and 0.54 ± 0.11 N in right limb. These results not only showed that rehabilitation enhanced recovery of motor function, but also demonstrated the viability of measuring the ground reaction force applied by the rats as an assessment for a full spinal cord transection injury model.
keywords: {biomechanics;biomedical measurement;force sensors;injuries;medical disorders;molecular biophysics;nanofibres;nanomedicine;neurophysiology;patient rehabilitation;patient treatment;proteins;tissue engineering;T9-T10 level;human observance;nanofiber scaffold;neurotrophin-3;right limb;force sensing system;spinalized rats;force detection;effective treatment method;spinal cord injury;complete spinal cord;spinal cord transection injury model;motor function;rehabilitation enhanced recovery;rehabilitated rats;left limb;ground reaction force;Rats;Force;Force measurement;Spinal cord;Robot sensing systems},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794368&isnumber=8793254
